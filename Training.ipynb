{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "familiar-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "controversial-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "olympic-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(root_path: str = \"/tmp\"):\n",
    "# data_path = \"./\"\n",
    "# if True:\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    join_path = lambda x: os.path.join(root_path, x)\n",
    "    \n",
    "    ## download from gcp bucket\n",
    "    bucket_name = \"ncedc\"\n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        print(f\"download {source_blob_name} to {destination_file_name}.\")\n",
    "        \n",
    "    if not os.path.exists(join_path(\"catalogs/\")):\n",
    "        os.mkdir(join_path(\"catalogs/\"))\n",
    "    download_blob(\"ncedc\", \"catalogs/combined_phases.csv\", join_path(\"catalogs/combined_phases.csv\"))\n",
    "    catalog = pd.read_csv(join_path(\"catalogs/combined_phases.csv\"), sep=\"\\t\")\n",
    "    \n",
    "    index = catalog[\"event_index\"]\n",
    "    test = catalog[ index >= (index.max()-1) ]\n",
    "    valid = catalog[ (index < (index.max()-1)) & (index >= (index.max()-2)) ]\n",
    "    train = catalog[ (index < (index.max()-2)) & (index >= (index.max()-10)) ]\n",
    "    \n",
    "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
    "        print(f\"Downloading {dataset} dataset...\")\n",
    "        eval(dataset).to_csv(f\"{dataset}.csv\", sep=\"\\t\")\n",
    "        if not os.path.exists(join_path(f\"{dataset}_data/\")):\n",
    "            os.mkdir(join_path(f\"{dataset}_data/\"))\n",
    "        num = len(eval(dataset)[\"fname\"])\n",
    "        for i, fname in enumerate(eval(dataset)[\"fname\"]):\n",
    "            print(f\"{i+1}/{num}\",  end=' ')\n",
    "            download_blob(bucket_name, f\"data/{fname}\", join_path(f\"{dataset}_data/{fname}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solid-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(data_path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescribed-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset_op = comp.func_to_container_op(download_dataset, \n",
    "                                                base_image='python:3.7',\n",
    "                                                packages_to_install= [\n",
    "                                                    \"pandas\",\n",
    "                                                    \"google-cloud-storage\"\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dominant-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_op(root_path: str, \n",
    "                model_path: str,\n",
    "                batch_size: int = 1,\n",
    "                train_path: str = \"train_data\", \n",
    "                train_csv: str = \"train_csv\",\n",
    "                ):\n",
    "    \n",
    "    import os\n",
    "    join_path = lambda x: os.path.join(root_path, x)\n",
    "    return dsl.ContainerOp(name='PhaseNet training',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=[\"pwd\"],\n",
    "#                            command=['python'],\n",
    "#                            arguments=[\n",
    "#                                'train.py',\n",
    "#                                '--epoch', 10,\n",
    "#                                '--batch_size', batch_size,\n",
    "#                                '--train_dir', join_path(train_path),\n",
    "#                                '--train_list', join_path(train_csv),\n",
    "#                                '--log_dir', model_path\n",
    "#                                ],\n",
    "#                            file_outputs = {\"model_path\": model_path}\n",
    "                           file_outputs = {\"model_path\": \"model/190703-214543\"}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinated-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python PhaseNet/phasenet/train.py --epoch=10 --batch_size=1 --log_dir=models --train_list=train.csv --train_dir=train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extraordinary-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_op(root_path: str, \n",
    "            model_path: str,\n",
    "            batch_size: int = 1,\n",
    "            test_path: str = \"test_data\", \n",
    "            test_csv: str = \"test_csv\",\n",
    "            result_path: str = \"results\"):\n",
    "    \n",
    "    import os\n",
    "    join_path = lambda x: os.path.join(root_path, x)\n",
    "    return dsl.ContainerOp(name='PhaseNet test',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=[\"pwd\"],\n",
    "#                            command=['python'],\n",
    "#                            arguments=[\n",
    "#                                'train.py',\n",
    "#                                '--mode', \"test\",\n",
    "#                                '--model', model_path,\n",
    "#                                '--batch_size', batch_size,\n",
    "#                                '--train_dir', join_path(test_path),\n",
    "#                                '--train_list', join_path(test_csv),\n",
    "#                                '--log_dir', result_path\n",
    "#                                ],\n",
    "#                            file_outputs = {\"result\": f\"{result_path}\"}\n",
    "                           file_outputs = {\"result\": \"model/190703-214543/loss.log\"}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prerequisite-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python PhaseNet/phasenet/train.py --mode=test --model=PhaseNet/model/190703-214543 --batch_size=1 --log_dir=results --test_list=test.csv --test_dir=test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "likely-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model(root_path: str,\n",
    "                 local_path: str,\n",
    "                 remote_path:str = \"phaset\",\n",
    "                 bucket_name:str = \"models\",\n",
    "                 s3_url:str = \"localhost:9000\", \n",
    "                 secure:bool = True):\n",
    "    \n",
    "    import os\n",
    "    from minio import Minio\n",
    "    minioClient = Minio(s3_url,\n",
    "                  access_key='minio',\n",
    "                  secret_key='minio123',\n",
    "                  secure=secure)\n",
    "    \n",
    "    for f in os.listdir(os.path.join(root_path, local_path)):\n",
    "        if os.path.isfile(os.path.join(root_path, local_path, f)):\n",
    "            minioClient.fput_object(bucket_name, os.path.join(remote_path, f), os.path.join(root_path, local_path, f))\n",
    "            print(f\"upload {os.path.join(root_path, local_path, f)} to {os.path.join(remote_path, f)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "touched-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_model(\"./\", \"PhaseNet/model/190703-214543\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "involved-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model_op = comp.func_to_container_op(upload_model, \n",
    "                                            base_image='python:3.7',\n",
    "                                            packages_to_install= [\n",
    "                                                \"pandas\",\n",
    "                                                \"minio\"\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mechanical-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quakeflow_training(root_path:str,  bucket_name:str, s3_url:str, secure:bool):\n",
    "    \n",
    "    vop_ = dsl.VolumeOp(name=\"Create_volume\",\n",
    "                       resource_name=\"data-volume\", \n",
    "                       size=\"10Gi\", \n",
    "                       modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    \n",
    "    download_ = download_dataset_op(root_path).add_pvolumes({root_path: vop_.volume})\n",
    "    \n",
    "    train_ = training_op(root_path, model_path=\"models\").add_pvolumes({root_path: download_.pvolume})\n",
    "    \n",
    "    test_ = test_op(root_path, model_path=train_.outputs[\"model_path\"], result_path=\"model/190703-214543\").add_pvolumes({root_path: train_.pvolume})\n",
    "    \n",
    "#     with dsl.Condition(test_.output > 0.9):\n",
    "    upload_ = upload_model_op(root_path, local_path=train_.outputs[\"model_path\"]).add_pvolumes({root_path: test_.pvolume})\n",
    "#         upload_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powerful-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='https://45ff9cb0ebef.ngrok.io')\n",
    "# client = kfp.Client(host='127.0.0.1:8080')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stuck-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://45ff9cb0ebef.ngrok.io/#/experiments/details/7a3583d2-fd6d-4bf5-b0dc-02030b9ac7fc\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://45ff9cb0ebef.ngrok.io/#/runs/details/21894c14-ddc9-48a0-8668-9c01321798c3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'QuakeFlow Training'\n",
    "pipeline_func = quakeflow_training\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\"root_path\": \"/tmp/\",\n",
    "             \"bucket_name\": \"catalogs\",\n",
    "             \"s3_url\": \"localhost:9000\",\n",
    "             \"secure\": False\n",
    "#              \"s3_url\": \"10.111.90.219:9000\",\n",
    "#              \"s3_url\": \"10.97.200.84:9000\",\n",
    "#              \"secure\": False\n",
    "             }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-antibody",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-affairs",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
