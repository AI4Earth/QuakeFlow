{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "demographic-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow_wayne.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "exceptional-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(root_path: str = \"/tmp\"):\n",
    "# data_path = \"./\"\n",
    "# if True:\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    join_path = lambda x: os.path.join(root_path, x)\n",
    "    \n",
    "    ## download from gcp bucket\n",
    "    bucket_name = \"ncedc\"\n",
    "    def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "        print(f\"download {source_blob_name} to {destination_file_name}.\")\n",
    "        \n",
    "    if not os.path.exists(join_path(\"catalogs/\")):\n",
    "        os.mkdir(join_path(\"catalogs/\"))\n",
    "    download_blob(\"ncedc\", \"catalogs/combined_phases.csv\", join_path(\"catalogs/combined_phases.csv\"))\n",
    "    catalog = pd.read_csv(join_path(\"catalogs/combined_phases.csv\"), sep=\"\\t\")\n",
    "    \n",
    "    index = catalog[\"event_index\"]\n",
    "    test = catalog[ index >= (index.max()-1) ]\n",
    "    valid = catalog[ (index < (index.max()-1)) & (index >= (index.max()-2)) ]\n",
    "    train = catalog[ (index < (index.max()-2)) & (index >= (index.max()-10)) ]\n",
    "    \n",
    "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
    "        print(f\"Downloading {dataset} dataset...\")\n",
    "#         eval(dataset).to_csv(join_path(f\"{dataset}.csv\"), sep=\"\\t\")\n",
    "        eval(dataset)[\"itp\"] = eval(dataset)[\"p_idx\"]\n",
    "        eval(dataset)[\"its\"] = eval(dataset)[\"s_idx\"]\n",
    "        eval(dataset).to_csv(join_path(f\"{dataset}.csv\"))\n",
    "        print(f\"Save {join_path(f'{dataset}.csv')}\")\n",
    "        if not os.path.exists(join_path(f\"{dataset}_data/\")):\n",
    "            os.mkdir(join_path(f\"{dataset}_data/\"))\n",
    "        num = len(eval(dataset)[\"fname\"])\n",
    "        for i, fname in enumerate(eval(dataset)[\"fname\"]):\n",
    "            print(f\"{i+1}/{num}\",  end=' ')\n",
    "            download_blob(bucket_name, f\"data/{fname}\", join_path(f\"{dataset}_data/{fname}\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ancient-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(root_path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "informal-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset_op = comp.func_to_container_op(download_dataset, \n",
    "                                                base_image='python:3.7',\n",
    "                                                packages_to_install= [\n",
    "                                                    \"pandas\",\n",
    "                                                    \"google-cloud-storage\"\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "vanilla-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_op(root_path: str, \n",
    "                model_path: str,\n",
    "                batch_size: int = 1,\n",
    "                train_path: str = \"train_data\", \n",
    "                train_csv: str = \"train_csv\",\n",
    "                ):\n",
    "    \n",
    "    import os\n",
    "    join_path = lambda x: os.path.join(str(root_path), x)\n",
    "    return dsl.ContainerOp(name='PhaseNet training',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=['sh', '-c'],\n",
    "                           arguments=[\"du -h /opt/model/190703-214543\"],\n",
    "#                            command=['python'],\n",
    "#                            arguments=[\n",
    "#                                'train.py',\n",
    "#                                '--epoch', 10,\n",
    "#                                '--batch_size', batch_size,\n",
    "#                                '--train_dir', join_path(str(train_path)),\n",
    "#                                '--train_list', join_path(str(train_csv)),\n",
    "#                                '--log_dir', model_path\n",
    "#                                ],\n",
    "#                            file_outputs = {\"model_path\": model_path}\n",
    "#                            file_outputs = {\"model_path\": \"/opt/model/190703-214543\"}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "european-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python PhaseNet/phasenet/train.py --epoch=10 --batch_size=1 --log_dir=models --train_list=train.csv --train_dir=train_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "illegal-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_op(root_path: str, \n",
    "            model_path: str,\n",
    "            batch_size: int = 1,\n",
    "            test_path: str = \"test_data\", \n",
    "            test_csv: str = \"test.csv\",\n",
    "            result_path: str = \"results\"):\n",
    "    \n",
    "    import os\n",
    "    join_path = lambda x: os.path.join(str(root_path), x)\n",
    "    return dsl.ContainerOp(name='PhaseNet test',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=['sh', '-c'],\n",
    "                           arguments=[\"ls ./\"],\n",
    "#                            command=['python'],\n",
    "#                            arguments=[\n",
    "#                                'train.py',\n",
    "#                                '--mode', \"test\",\n",
    "#                                '--model', model_path,\n",
    "#                                '--batch_size', batch_size,\n",
    "#                                '--test_dir', join_path(str(test_path)),\n",
    "#                                '--test_list', join_path(str(test_csv)),\n",
    "#                                '--log_dir', result_path\n",
    "#                                ],\n",
    "#                            file_outputs = {\"result\": f\"{result_path}/loss.log\"}\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "critical-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python PhaseNet/phasenet/train.py --mode=test --model=PhaseNet/model/190703-214543 --batch_size=1 --log_dir=results --test_list=test.csv --test_dir=test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "renewable-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model(root_path: str,\n",
    "                 local_path: str,\n",
    "                 remote_path:str = \"phasenet\",\n",
    "                 bucket_name:str = \"models\",\n",
    "                 s3_url:str = \"localhost:9000\", \n",
    "                 secure:bool = True):\n",
    "    \n",
    "    import os\n",
    "    from minio import Minio\n",
    "    minioClient = Minio(s3_url,\n",
    "                  access_key='minio',\n",
    "                  secret_key='minio123',\n",
    "                  secure=secure)\n",
    "    if not minioClient.bucket_exists(bucket_name):\n",
    "        minioClient.make_bucket(bucket_name)\n",
    "    \n",
    "    for f in os.listdir(os.path.join(root_path, local_path)):\n",
    "        if os.path.isfile(os.path.join(root_path, local_path, f)):\n",
    "            minioClient.fput_object(bucket_name, os.path.join(remote_path, f), os.path.join(root_path, local_path, f))\n",
    "            print(f\"upload {os.path.join(root_path, local_path, f)} to {os.path.join(remote_path, f)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "stunning-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_model(\"./\", \"PhaseNet/model/190703-214543\", s3_url=\"localhost:9000\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fatty-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_model_op = comp.func_to_container_op(upload_model, \n",
    "                                            base_image='python:3.7',\n",
    "                                            packages_to_install= [\n",
    "                                                \"pandas\",\n",
    "                                                \"minio\"\n",
    "                                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "happy-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quakeflow_training(root_path:str, model_path:str, bucket_name:str, s3_url:str, secure:bool):\n",
    "    \n",
    "    vop_ = dsl.VolumeOp(name=\"Create_volume\", resource_name=\"data-volume\", size=\"10Gi\", modes=dsl.VOLUME_MODE_RWO).set_display_name('Persistent Volume')\n",
    "    \n",
    "    download_ = download_dataset_op(root_path).add_pvolumes({root_path: vop_.volume}).set_display_name('Download Datasets')\n",
    "    \n",
    "    train_ = training_op(root_path, model_path).add_pvolumes({root_path: download_.pvolume}).set_display_name('Training')\n",
    "    \n",
    "    test_ = test_op(root_path, model_path=\"model/190703-214543\", result_path=\"model/190703-214543\").add_pvolumes({root_path: train_.pvolume}).set_display_name('Inference')\n",
    "    \n",
    "#     with dsl.Condition(test_.output > 0.9):\n",
    "    upload_ = upload_model_op(root_path, \"model/190703-214543/\", s3_url=s3_url, secure=secure).add_pvolumes({root_path: test_.pvolume}).set_display_name('Upload Model')\n",
    "#         upload_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "demonstrated-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = kfp.Client(host='https://45ff9cb0ebef.ngrok.io')\n",
    "client = kfp.Client(host=\"50e8cc1eb0348bb3-dot-us-west1.pipelines.googleusercontent.com\")\n",
    "# client = kfp.Client(host='127.0.0.1:8080')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "loved-notice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://50e8cc1eb0348bb3-dot-us-west1.pipelines.googleusercontent.com/#/experiments/details/e36515d1-4821-417c-97a4-7a5a799976af\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://50e8cc1eb0348bb3-dot-us-west1.pipelines.googleusercontent.com/#/runs/details/9c1aef48-1cc4-4945-a7f2-dc1cc8e45db7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'QuakeFlow Training'\n",
    "pipeline_func = quakeflow_training\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\"root_path\": \"/tmp/\",\n",
    "             \"model_path\": \"models\",\n",
    "             \"bucket_name\": \"catalogs\",\n",
    "#              \"s3_url\": \"localhost:9000\",\n",
    "#              \"secure\": False\n",
    "             \"s3_url\": \"10.3.254.67:9000\",\n",
    "             \"secure\": False\n",
    "             }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-premises",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-contribution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-africa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
