{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone -b factorize https://github.com/wayneweiqiang/PhaseNet.git\n",
    "# !git clone https://github.com/wayneweiqiang/GMMA.git\n",
    "# !conda env update -f=env.yml -n base\n",
    "\n",
    "## or install to venv\n",
    "# !conda env create -f=env.yml -n quakeflow --force\n",
    "# !python -m ipykernel install --user --name=quakeflow\n",
    "## select jupyter notebook kernel to quakflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "# matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# region_name = \"SaltonSea\"\n",
    "region_name = \"Ridgecrest\"\n",
    "# region_name = \"SanSimeon\"\n",
    "# region_name = \"Italy\"\n",
    "# region_name = \"PNSN\"\n",
    "# region_name = \"Hawaii\"\n",
    "# region_name = \"PuertoRico\"\n",
    "dir_name = region_name\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "root_dir = lambda x: os.path.join(dir_name, x)\n",
    "\n",
    "run_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(index_pkl: OutputPath(\"pickle\"),\n",
    "               config_pkl: OutputPath(\"pickle\"), \n",
    "               datetime_pkl: OutputPath(\"pickle\"),\n",
    "               num_parallel:int = 1) -> list:\n",
    "    \n",
    "    import obspy\n",
    "    import os\n",
    "    import pickle\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import json\n",
    "    \n",
    "    pi = 3.1415926\n",
    "    degree2km = pi*6371/180\n",
    "    \n",
    "#     ## Data center\n",
    "#     client = \"IRIS\"\n",
    "# #     client = \"SCEDC\"\n",
    "# #     client = \"NCEDC\"\n",
    "# #     client = \"INGV\"\n",
    "# #     client = \"*\"\n",
    "    \n",
    "#     ## Seismic stations\n",
    "#     network_list = [\"*\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "    \n",
    "    region_name = \"Ridgecrest\"\n",
    "    center = (-117.504, 35.705)\n",
    "    horizontal_degree = 1.0\n",
    "    vertical_degree = 1.0\n",
    "    starttime = obspy.UTCDateTime(\"2019-07-04T17\") # Debug Ridgecrest\n",
    "    endtime = obspy.UTCDateTime(\"2019-07-05T00\")\n",
    "#     starttime = obspy.UTCDateTime(\"2019-07-04T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2019-07-11T00\")\n",
    "    client = \"SCEDC\"\n",
    "    network_list = [\"CI\"]\n",
    "    channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "    \n",
    "#     region_name = \"Hawaii\"\n",
    "#     center = (-155.32, 19.39)\n",
    "#     horizontal_degree = 2.0\n",
    "#     vertical_degree = 2.0\n",
    "#     starttime = obspy.UTCDateTime(\"2021-04-01T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2021-05-01T00\") \n",
    "# #     endtime = obspy.UTCDateTime(\"2021-04-01T03\") \n",
    "#     client = \"IRIS\"\n",
    "#     network_list = [\"HV\", \"PT\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "    \n",
    "#     region_name = \"PuertoRico\"\n",
    "#     center = (-66.5, 18)\n",
    "#     horizontal_degree = 3.0\n",
    "#     vertical_degree = 2.0\n",
    "#     starttime = obspy.UTCDateTime(\"2020-01-07T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2020-01-14T00\") \n",
    "# #     endtime = obspy.UTCDateTime(\"2021-04-01T03\") \n",
    "#     client = \"IRIS\"\n",
    "#     network_list = [\"*\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "#     region_name = \"SaltonSea\"\n",
    "#     center = (-115.53, 32.98)\n",
    "#     horizontal_degree = 1.0\n",
    "#     vertical_degree = 1.0\n",
    "#     starttime = obspy.UTCDateTime(\"2020-10-01T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2020-10-01T02\")\n",
    "#     client = \"SCEDC\"\n",
    "#     network_list = [\"CI\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "#     region_name = \"2003SanSimeon\"\n",
    "#     center = (-121.101, 35.701)\n",
    "#     horizontal_degree = 1.0\n",
    "#     vertical_degree = 1.0\n",
    "#     starttime = obspy.UTCDateTime(\"2003-12-22T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2003-12-24T00\") \n",
    "#     client = \"NCEDC\"\n",
    "#     network_list = [\"*\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "    \n",
    "#     region_name = \"Italy\"\n",
    "#     center = (13.188, 42.723)\n",
    "#     horizontal_degree = 1.0\n",
    "#     vertical_degree = 1.0\n",
    "#     starttime = obspy.UTCDateTime(\"2016-08-24T00\")\n",
    "#     endtime = obspy.UTCDateTime(\"2016-08-26T00\")\n",
    "#     client = \"INGV\"\n",
    "#     network_list = [\"*\"]\n",
    "#     channel_list = \"HH*,BH*,EH*,HN*\"\n",
    "\n",
    "\n",
    "    ####### save config ########\n",
    "    config = {}\n",
    "    config[\"region\"] = region_name\n",
    "    config[\"center\"] = center\n",
    "    config[\"xlim_degree\"] = [center[0]-horizontal_degree/2, center[0]+horizontal_degree/2]\n",
    "    config[\"ylim_degree\"] = [center[1]-vertical_degree/2, center[1]+vertical_degree/2]\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime.datetime.isoformat()\n",
    "    config[\"endtime\"] = endtime.datetime.isoformat()\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"client\"] = client\n",
    "\n",
    "#     with open(config_pkl, \"wb\") as fp:\n",
    "#         pickle.dump(config, fp)\n",
    "    with open(config_pkl, 'w') as fp:\n",
    "        json.dump(config, fp)\n",
    "        \n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    one_hour = datetime.timedelta(hours=1)\n",
    "    starttimes = []\n",
    "    tmp_start = starttime\n",
    "    while tmp_start < endtime:\n",
    "        starttimes.append(tmp_start.datetime.isoformat())\n",
    "        tmp_start += one_hour\n",
    "    \n",
    "#     with open(datetime_pkl, \"wb\") as fp:\n",
    "#         pickle.dump({\"starttimes\": starttimes, \"interval\": one_hour}, fp)\n",
    "    with open(datetime_pkl, \"w\") as fp:\n",
    "        json.dump({\"starttimes\": starttimes, \"interval\": one_hour.total_seconds()}, fp)\n",
    "        \n",
    "    if num_parallel == 0:\n",
    "#         num_parallel = (len(starttimes)-1)//(24*3)+1\n",
    "        num_parallel = min(8, len(starttimes))\n",
    "    \n",
    "    idx = [[] for i in range(num_parallel)]\n",
    "    for i in range(len(starttimes)):\n",
    "        idx[i - i//num_parallel*num_parallel].append(i)\n",
    "        \n",
    "#     with open(index_pkl, \"wb\") as fp:\n",
    "#         pickle.dump(idx, fp)\n",
    "    with open(index_pkl, 'w') as fp:\n",
    "        json.dump(idx, fp)\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    idx = set_config(root_dir(\"index.pkl\"), root_dir(\"config.pkl\"), root_dir(\"datetimes.pkl\"), num_parallel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_op = comp.func_to_container_op(set_config, \n",
    "                                      base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                       base_image='python:3.8',\n",
    "#                                       packages_to_install= [\n",
    "#                                           \"numpy\",\n",
    "#                                           \"obspy\",\n",
    "#                                       ]\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_events(config_pkl: InputPath(\"pickle\"),\n",
    "                    event_csv: OutputPath(str)):\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import matplotlib\n",
    "#     matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "#     with open(config_pkl, \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_pkl, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    \n",
    "    ####### IRIS catalog ########\n",
    "    try:\n",
    "        events = Client(config[\"client\"]).get_events(starttime=config[\"starttime\"],\n",
    "                                           endtime=config[\"endtime\"],\n",
    "                                           minlongitude=config[\"xlim_degree\"][0],\n",
    "                                           maxlongitude=config[\"xlim_degree\"][1],\n",
    "                                           minlatitude=config[\"ylim_degree\"][0],\n",
    "                                           maxlatitude=config[\"ylim_degree\"][1])#,\n",
    "    #                                        filename='events.xml')\n",
    "    except:\n",
    "        events = Client(\"iris\").get_events(starttime=config[\"starttime\"],\n",
    "                                   endtime=config[\"endtime\"],\n",
    "                                   minlongitude=config[\"xlim_degree\"][0],\n",
    "                                   maxlongitude=config[\"xlim_degree\"][1],\n",
    "                                   minlatitude=config[\"ylim_degree\"][0],\n",
    "                                   maxlatitude=config[\"ylim_degree\"][1])#,\n",
    "#                                        filename='events.xml')\n",
    "\n",
    "#     events = obspy.read_events('events.xml')\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "#     events.plot('local', outfile=\"events.png\")\n",
    "#     events.plot('local')\n",
    "\n",
    "    ####### Save catalog ########\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        if len(event.magnitudes) > 0:\n",
    "            catalog[\"time\"].append(event.origins[0].time.datetime)\n",
    "            catalog[\"magnitude\"].append(event.magnitudes[0].mag)\n",
    "            catalog[\"longitude\"].append(event.origins[0].longitude)\n",
    "            catalog[\"latitude\"].append(event.origins[0].latitude)\n",
    "            catalog[\"depth(m)\"].append(event.origins[0].depth)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "    catalog.to_csv(event_csv,\n",
    "                   sep=\"\\t\", index=False, float_format=\"%.3f\",\n",
    "                   date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                   columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\"])\n",
    "\n",
    "    ####### Plot catalog ########\n",
    "    plt.figure()\n",
    "    plt.plot(catalog[\"longitude\"], catalog[\"latitude\"], '.', markersize=1)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "#     plt.savefig(os.path.join(data_path, \"events_loc.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot_date(catalog[\"time\"], catalog[\"magnitude\"], '.', markersize=1)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(f\"Number of events: {len(events)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"events_mag_time.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_events(root_dir(\"config.pkl\"), root_dir(\"events.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_events_op = comp.func_to_container_op(download_events, \n",
    "                                               base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                                base_image='python:3.8',\n",
    "#                                                packages_to_install= [\n",
    "#                                                   \"obspy\",\n",
    "#                                                   \"pandas\",\n",
    "#                                                   \"matplotlib\",\n",
    "#                                                ]\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations(config_pkl: InputPath(\"pickle\"),\n",
    "                      station_csv: OutputPath(str),\n",
    "                      station_pkl: OutputPath(\"pickle\")):\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import matplotlib\n",
    "#     matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "#     with open(config_pkl, \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_pkl, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "\n",
    "    print(\"Network:\", \",\".join(config[\"networks\"]))\n",
    "    ####### Download stations ########\n",
    "    stations = Client(config[\"client\"]).get_stations(network = \",\".join(config[\"networks\"]),\n",
    "                                           station = \"*\",\n",
    "                                           starttime=config[\"starttime\"],\n",
    "                                           endtime=config[\"endtime\"],\n",
    "                                           minlongitude=config[\"xlim_degree\"][0],\n",
    "                                           maxlongitude=config[\"xlim_degree\"][1],\n",
    "                                           minlatitude=config[\"ylim_degree\"][0],\n",
    "                                           maxlatitude=config[\"ylim_degree\"][1],\n",
    "                                           channel=config[\"channels\"],\n",
    "                                           level=\"response\")#,\n",
    "#                                            filename=\"stations.xml\")\n",
    "\n",
    "#     stations = obspy.read_inventory(\"stations.xml\")\n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "    # stations.plot('local', outfile=\"stations.png\")\n",
    "#     stations.plot('local')\n",
    "    \n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                    station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                else:\n",
    "                    component = f\"{chn.code[-1]}\"\n",
    "                    response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                    dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict[\"longitude\"], tmp_dict[\"latitude\"], tmp_dict[\"elevation(m)\"] = chn.longitude, chn.latitude, chn.elevation\n",
    "                    tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"unit\"] = component, response, dtype\n",
    "                    station_locs[sid] = tmp_dict\n",
    "                    \n",
    "    station_locs = pd.DataFrame.from_dict(station_locs, orient='index')\n",
    "    station_locs.to_csv(station_csv,\n",
    "                        sep=\"\\t\", float_format=\"%.3f\",\n",
    "                        index_label=\"station\",\n",
    "                        columns=[\"longitude\", \"latitude\", \"elevation(m)\", \"unit\", \"component\", \"response\"])\n",
    "\n",
    "    with open(station_pkl, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "#     with open(station_pkl, \"w\") as fp:\n",
    "#         json.dump(stations, fp)\n",
    "        \n",
    "#     ####### Plot stations ########\n",
    "    plt.figure()\n",
    "    plt.plot(station_locs[\"longitude\"], station_locs[\"latitude\"], \"^\", label=\"Stations\")\n",
    "    plt.xlabel(\"X (km)\")\n",
    "    plt.ylabel(\"Y (km)\")\n",
    "    plt.axis(\"scaled\")\n",
    "    plt.xlim(config[\"xlim_degree\"])\n",
    "    plt.ylim(config[\"ylim_degree\"])\n",
    "    plt.legend()\n",
    "    plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"stations_loc.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    download_stations(root_dir(\"config.pkl\"), root_dir(\"stations.csv\"), root_dir(\"stations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_stations_op = comp.func_to_container_op(download_stations, \n",
    "                                                 base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                                  base_image='python:3.8',\n",
    "#                                                  packages_to_install= [\n",
    "#                                                      \"obspy\",\n",
    "#                                                      \"pandas\",\n",
    "#                                                      \"matplotlib\",\n",
    "#                                                  ]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waveform(i: int, \n",
    "                      index_pkl: InputPath(\"pickle\"),\n",
    "                      config_pkl: InputPath(\"pickle\"),\n",
    "                      datetime_pkl: InputPath(\"pickle\"),\n",
    "                      station_pkl: InputPath(\"pickle\"),\n",
    "                      fname_csv: OutputPath(str),\n",
    "                      data_path:str,\n",
    "#                       bucket_name:str = \"waveforms\",\n",
    "#                       s3_url:str = \"localhost:9000\", \n",
    "#                       secure:bool = True\n",
    "                     ) -> str:\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import time\n",
    "    import json\n",
    "    import random\n",
    "    import threading\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "#     from minio import Minio\n",
    "#     minioClient = Minio(s3_url,\n",
    "#                   access_key='minio',\n",
    "#                   secret_key='minio123',\n",
    "#                   secure=secure)\n",
    "    \n",
    "#     if not minioClient.bucket_exists(bucket_name):\n",
    "#         minioClient.make_bucket(bucket_name)\n",
    "\n",
    "#     with open(index_pkl, \"rb\") as fp:\n",
    "#         index = pickle.load(fp)\n",
    "    with open(index_pkl, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "#     with open(config_pkl, \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_pkl, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "#     with open(datetime_pkl, \"rb\") as fp:\n",
    "#         tmp = pickle.load(fp)\n",
    "#         starttimes = tmp[\"starttimes\"]\n",
    "#         interval = tmp[\"interval\"]\n",
    "    with open(datetime_pkl, \"r\") as fp:\n",
    "        tmp = json.load(fp)\n",
    "        starttimes = tmp[\"starttimes\"]\n",
    "        interval = tmp[\"interval\"]\n",
    "    with open(station_pkl, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "#     with open(station_pkl, \"r\") as fp:\n",
    "#         stations = json.load(fp)\n",
    "    \n",
    "    waveform_dir = os.path.join(data_path, config[\"region\"], \"waveforms\")\n",
    "    if not os.path.exists(waveform_dir):\n",
    "        os.makedirs(waveform_dir)\n",
    "        \n",
    "    ####### Download data ########\n",
    "    client = Client(config[\"client\"])\n",
    "    fname_list = [\"fname\"]\n",
    "    \n",
    "    def download(i):\n",
    "#     for i in idx: \n",
    "        starttime = obspy.UTCDateTime(starttimes[i]) \n",
    "        endtime = starttime + interval\n",
    "        fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "        if os.path.exists(os.path.join(waveform_dir, fname)):\n",
    "            print(f\"{fname} exists\")\n",
    "            fname_list.append(fname)\n",
    "            return\n",
    "        max_retry = 10\n",
    "        stream = obspy.Stream()\n",
    "        print(f\"{fname} download starts\")\n",
    "        for network in stations:\n",
    "            for station in network:\n",
    "                print(f\"********{network.code}.{station.code}********\")\n",
    "                retry = 0\n",
    "                while retry < max_retry:\n",
    "                    try:\n",
    "                        tmp = client.get_waveforms(network.code, station.code, \"*\", config[\"channels\"], starttime, endtime)\n",
    "#                         for trace in tmp:\n",
    "#                             if trace.stats.sampling_rate != 100:\n",
    "#                                 print(trace)\n",
    "#                                 trace = trace.interpolate(100, method=\"linear\")\n",
    "#                             trace = trace.detrend(\"spline\", order=2, dspline=5*trace.stats.sampling_rate)\n",
    "#                             stream.append(trace)\n",
    "                        stream += tmp\n",
    "                        break\n",
    "                    except Exception as err:\n",
    "                        print(\"Error {}.{}: {}\".format(network.code, station.code, err))\n",
    "                        message = \"No data available for request.\"\n",
    "                        if str(err)[:len(message)] == message:\n",
    "                            break\n",
    "                        retry += 1\n",
    "                        time.sleep(5)\n",
    "                        continue\n",
    "                if retry == max_retry:\n",
    "                    print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code}\")\n",
    "        \n",
    "#         stream = stream.merge(fill_value=0)\n",
    "#         stream = stream.trim(starttime, endtime, pad=True, fill_value=0)\n",
    "        \n",
    "        stream.write(os.path.join(waveform_dir, fname))\n",
    "        print(f\"{fname} download succeeds\")\n",
    "#         minioClient.fput_object(bucket_name, fname, os.path.join(waveform_dir, fname))\n",
    "        lock.acquire()\n",
    "        fname_list.append(fname)\n",
    "        lock.release()\n",
    "    \n",
    "    threads = []\n",
    "    MAX_THREADS = 4\n",
    "    for ii, i in enumerate(idx):\n",
    "        t = threading.Thread(target=download, args=(i,))\n",
    "        t.start()\n",
    "        time.sleep(1)\n",
    "        threads.append(t)\n",
    "        if ii%MAX_THREADS == MAX_THREADS-1:\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            threads = []\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    with open(fname_csv, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(fname_list))\n",
    "\n",
    "    return waveform_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    waveform_path = download_waveform(0, root_dir(\"index.pkl\"), root_dir(\"config.pkl\"), root_dir(\"datetimes.pkl\"), root_dir(\"stations.pkl\"), root_dir(\"fname.csv\"), data_path=root_dir(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_waveform_op = comp.func_to_container_op(download_waveform,\n",
    "                                                 base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                                  base_image='python:3.8',\n",
    "#                                                  packages_to_install= [\n",
    "#                                                      \"obspy\",\n",
    "# #                                                      \"minio\"\n",
    "#                                                  ]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phasenet_op(data_path: str, \n",
    "                data_list: str, \n",
    "                stations: str):\n",
    "\n",
    "    return dsl.ContainerOp(name='PhaseNet Picking',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=['python'],\n",
    "                           arguments=[\n",
    "                             'predict.py',\n",
    "                             '--model', \"model/190703-214543\",\n",
    "                             '--data_dir', data_path,\n",
    "                             '--data_list', dsl.InputArgumentPath(data_list),\n",
    "                             '--stations', dsl.InputArgumentPath(stations),\n",
    "#                              '--result_dir', \"results\",\n",
    "                             '--format', \"mseed_array\",\n",
    "                             '--amplitude'\n",
    "                             ],\n",
    "                           file_outputs = {\"picks\": \"/opt/results/picks.json\"}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_local:\n",
    "    command = f\"python PhaseNet/phasenet/predict.py --model=PhaseNet/model/190703-214543 --data_dir={root_dir(root_dir('waveforms'))} --data_list={root_dir('fname.csv')} --stations={root_dir('stations.csv')} --result_dir={root_dir('phasenet')} --format=mseed_array --amplitude\"\n",
    "    print(command)\n",
    "    !{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmma(i: int,\n",
    "         index_pkl: InputPath(\"pickle\"),\n",
    "         config_pkl: InputPath(\"pickle\"),\n",
    "         pick_json: InputPath(\"json\"),\n",
    "         station_csv: InputPath(str),\n",
    "         catalog_csv: OutputPath(str),\n",
    "         picks_csv: OutputPath(str),\n",
    "         bucket_name:str = \"catalogs\",\n",
    "         s3_url:str = \"localhost:9000\", \n",
    "         secure:bool = True) -> str:\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from gmma import mixture\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import DBSCAN \n",
    "    from datetime import datetime, timedelta\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    to_seconds = lambda t: t.timestamp(tz=\"UTC\")\n",
    "    from_seconds = lambda t: pd.Timestamp.utcfromtimestamp(t).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]\n",
    "    # to_seconds = lambda t: datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\").timestamp()\n",
    "    # from_seconds = lambda t: [datetime.utcfromtimestamp(x).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] for x in t]\n",
    "\n",
    "    def convert_picks_csv(picks, stations, config):\n",
    "        t = picks[\"timestamp\"].apply(lambda x: x.timestamp()).to_numpy()\n",
    "        a = picks[\"amp\"].apply(lambda x: np.log10(x*1e2)).to_numpy()\n",
    "        data = np.stack([t, a]).T\n",
    "        meta = pd.merge(stations, picks[\"id\"], on=\"id\")\n",
    "        locs = meta[config[\"dims\"]].to_numpy()\n",
    "        phase_type = picks[\"type\"].apply(lambda x: x.lower()).to_numpy()\n",
    "        phase_weight = picks[\"prob\"].to_numpy()[:,np.newaxis]\n",
    "        return data, locs, phase_type, phase_weight\n",
    "\n",
    "    def association(data, locs, phase_type, phase_weight, num_sta, pick_idx, event_idx0, config, pbar=None):\n",
    "\n",
    "        db = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(np.hstack([data[:,0:1], locs[:,:2]/6.0]))#.fit(data[:,0:1])\n",
    "        labels = db.labels_\n",
    "        unique_labels = set(labels)\n",
    "        events = []\n",
    "        preds = []\n",
    "        probs = []\n",
    "\n",
    "        assignment = []\n",
    "        for k in unique_labels:\n",
    "            if k == -1:\n",
    "                continue\n",
    "\n",
    "            class_mask = (labels == k)\n",
    "            data_ = data[class_mask]\n",
    "            locs_ = locs[class_mask]\n",
    "            phase_type_ = phase_type[class_mask]\n",
    "            phase_weight_ = phase_weight[class_mask]\n",
    "            pick_idx_ = pick_idx[class_mask]\n",
    "            \n",
    "            if len(pick_idx_) <  config[\"min_picks_per_eq\"]:\n",
    "                continue\n",
    "\n",
    "            if pbar is not None:\n",
    "                pbar.set_description(f\"Process {len(data_)} picks\")\n",
    "\n",
    "            time_range = max(data_[:,0].max() - data_[:,0].min(), 1)\n",
    "\n",
    "            num_event_loc_init = 5\n",
    "            num_event_init = min(max(int(len(data_)/min(num_sta, 20) * config[\"oversample_factor\"]), 1), max(len(data_)//num_event_loc_init, 1))\n",
    "            x0, xn = config[\"x(km)\"]\n",
    "            y0, yn = config[\"y(km)\"]\n",
    "            x1, y1 = np.mean(config[\"x(km)\"]), np.mean(config[\"y(km)\"])\n",
    "            event_loc_init = [((x0+x1)/2, (y0+y1)/2), ((x0+x1)/2, (yn+y1)/2), ((xn+x1)/2, (y0+y1)/2), ((xn+x1)/2, (yn+y1)/2), (x1, y1)]\n",
    "            num_event_time_init = max(num_event_init//num_event_loc_init, 1)\n",
    "            centers_init = np.vstack([np.vstack([np.ones(num_event_time_init) * x, \n",
    "                                                 np.ones(num_event_time_init) * y,\n",
    "                                                 np.zeros(num_event_time_init),\n",
    "                                                 np.linspace(data_[:,0].min()-0.1*time_range, data_[:,0].max()+0.1*time_range, num_event_time_init)]).T\n",
    "                                      for x, y in event_loc_init])\n",
    "            \n",
    "            if len(data_) < len(centers_init):\n",
    "                num_event_init = min(max(int(len(data_)/min(num_sta, 20) * config[\"oversample_factor\"]), 1), len(data_))\n",
    "                centers_init = np.vstack([np.ones(num_event_init)*np.mean(stations[\"x(km)\"]),\n",
    "                                          np.ones(num_event_init)*np.mean(stations[\"y(km)\"]),\n",
    "                                          np.zeros(num_event_init),\n",
    "                                          np.linspace(data_[:,0].min()-0.1*time_range, data_[:,0].max()+0.1*time_range, num_event_init)]).T\n",
    "\n",
    "            mean_precision_prior = 0.1/time_range\n",
    "            if not config[\"use_amplitude\"]:\n",
    "                covariance_prior = np.array([[1]]) * 5\n",
    "                data_ = data_[:,0:1]\n",
    "            else:\n",
    "                covariance_prior = np.array([[1,0],[0,1]]) * 5\n",
    "\n",
    "            gmm = mixture.BayesianGaussianMixture(n_components=len(centers_init), \n",
    "                                                  weight_concentration_prior=1/len(centers_init),\n",
    "                                                  mean_precision_prior=mean_precision_prior,\n",
    "                                                  covariance_prior=covariance_prior,\n",
    "                                                  init_params=\"centers\",\n",
    "                                                  centers_init=centers_init.copy(), \n",
    "                                                  station_locs=locs_, \n",
    "                                                  phase_type=phase_type_, \n",
    "                                                  phase_weight=phase_weight_,\n",
    "                                                  loss_type=\"l1\",\n",
    "                                                  bounds=config[\"bfgs_bounds\"],\n",
    "                                                  max_covar=20**2,\n",
    "                                                  ).fit(data_) \n",
    "\n",
    "            pred = gmm.predict(data_) \n",
    "            prob_matrix = gmm.predict_proba(data_)\n",
    "            prob_eq = prob_matrix.mean(axis=0)\n",
    "    #             prob = prob_matrix[range(len(data_)), pred]\n",
    "    #             score = gmm.score(data_)\n",
    "    #             score_sample = gmm.score_samples(data_)\n",
    "            prob = np.exp(gmm.score_samples(data_))\n",
    "\n",
    "            idx = np.array([True if len(data_[pred==i, 0]) >= config[\"min_picks_per_eq\"] else False for i in range(len(prob_eq))]) #& (prob_eq > 1/num_event) #& (sigma_eq[:, 0,0] < 40)\n",
    "\n",
    "            time = gmm.centers_[idx, len(config[\"dims\"])]\n",
    "            loc = gmm.centers_[idx, :len(config[\"dims\"])]\n",
    "            if config[\"use_amplitude\"]:\n",
    "                mag = gmm.centers_[idx, len(config[\"dims\"])+1]\n",
    "            sigma_eq = gmm.covariances_[idx,...]\n",
    "\n",
    "            for i in range(len(time)):\n",
    "                tmp = {\"time(s)\": time[i],\n",
    "                       \"magnitude\": mag[i],\n",
    "                       \"sigma\": sigma_eq[i].tolist()}\n",
    "                for j, k in enumerate(config[\"dims\"]):\n",
    "                    tmp[k] = loc[i][j]\n",
    "                events.append(tmp)\n",
    "\n",
    "            for i in range(len(pick_idx_)):\n",
    "                assignment.append((pick_idx_[i], pred[i]+event_idx0, prob[i]))\n",
    "\n",
    "            event_idx0 += len(time)\n",
    "\n",
    "        return events, assignment\n",
    "    \n",
    "    catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "    if not os.path.exists(catalog_dir):\n",
    "        os.makedirs(catalog_dir)\n",
    "        \n",
    "#     with open(index_pkl, \"rb\") as fp:\n",
    "#         index = pickle.load(fp)\n",
    "    with open(index_pkl, \"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "    idx = index[i]\n",
    "\n",
    "#     with open(config_pkl, \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_pkl, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    \n",
    "    ## read picks\n",
    "    picks = pd.read_json(pick_json)\n",
    "    picks[\"time_idx\"] = picks[\"timestamp\"].apply(lambda x: x.strftime(\"%Y-%m-%dT%H\")) ## process by hours\n",
    "\n",
    "    ## read stations\n",
    "    stations = pd.read_csv(station_csv, delimiter=\"\\t\")\n",
    "    stations = stations.rename(columns={\"station\":\"id\"})\n",
    "    stations[\"x(km)\"] = stations[\"longitude\"].apply(lambda x: (x - config[\"center\"][0])*config[\"degree2km\"])\n",
    "    stations[\"y(km)\"] = stations[\"latitude\"].apply(lambda x: (x - config[\"center\"][1])*config[\"degree2km\"])\n",
    "    stations[\"z(km)\"] = stations[\"elevation(m)\"].apply(lambda x: -x/1e3)\n",
    "\n",
    "    ### setting GMMA configs\n",
    "    config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\n",
    "    config[\"use_dbscan\"] = True\n",
    "    config[\"use_amplitude\"] = True\n",
    "    config[\"x(km)\"] = (np.array(config[\"xlim_degree\"])-np.array(config[\"center\"][0]))*config[\"degree2km\"]\n",
    "    config[\"y(km)\"] = (np.array(config[\"ylim_degree\"])-np.array(config[\"center\"][1]))*config[\"degree2km\"]\n",
    "    config[\"z(km)\"] = (0, 40)\n",
    "    # DBSCAN\n",
    "    config[\"bfgs_bounds\"] = ((config[\"x(km)\"][0]-1, config[\"x(km)\"][1]+1), #x\n",
    "                            (config[\"y(km)\"][0]-1, config[\"y(km)\"][1]+1), #y\n",
    "                            (0, config[\"z(km)\"][1]+1), #x\n",
    "                            (None, None)) #t\n",
    "    config[\"dbscan_eps\"] = min(np.sqrt((stations[\"x(km)\"].max()-stations[\"x(km)\"].min())**2 +\n",
    "                                       (stations[\"y(km)\"].max()-stations[\"y(km)\"].min())**2)/(6.0/1.75), 6) #s\n",
    "    config[\"dbscan_min_samples\"] = min(len(stations), 3)\n",
    "    # Filtering\n",
    "    config[\"min_picks_per_eq\"] = min(len(stations)//2, 10)\n",
    "    config[\"oversample_factor\"] = min(len(stations)//2, 10)\n",
    "    print(\"Config: \", config)\n",
    "    \n",
    "    if config[\"use_amplitude\"]:\n",
    "        picks = picks[picks[\"amp\"]!=0]\n",
    "\n",
    "    ## run GMMA association\n",
    "    catalogs = []\n",
    "    pbar = tqdm(sorted(list(set(picks[\"time_idx\"]))))\n",
    "    event_idx0 = 0 ## current earthquake index\n",
    "    assignments = []\n",
    "    if (len(picks) > 0) and (len(picks) < 5000):\n",
    "        data, locs, phase_type, phase_weight = convert_picks_csv(picks, stations, config)\n",
    "        catalogs, assignments = association(data, locs, phase_type, phase_weight, len(stations), picks.index.to_numpy(), event_idx0, config, pbar)\n",
    "        event_idx0 += len(catalogs)\n",
    "    else:\n",
    "        for i, hour in enumerate(pbar):\n",
    "            picks_ = picks[picks[\"time_idx\"] == hour]\n",
    "            if len(picks_) == 0:\n",
    "                continue\n",
    "            data, locs, phase_type, phase_weight = convert_picks_csv(picks_, stations, config)\n",
    "            catalog, assign = association(data, locs, phase_type, phase_weight, len(stations), picks_.index.to_numpy(), event_idx0, config, pbar)\n",
    "            event_idx0 += len(catalog)\n",
    "            catalogs.extend(catalog)\n",
    "            assignments.extend(assign)\n",
    "\n",
    "    ## create catalog\n",
    "    catalogs = pd.DataFrame(catalogs, columns=[\"time(s)\"]+config[\"dims\"]+[\"magnitude\", \"sigma\"])\n",
    "    catalogs[\"time\"] = catalogs[\"time(s)\"].apply(lambda x: from_seconds(x))\n",
    "    catalogs[\"longitude\"] = catalogs[\"x(km)\"].apply(lambda x: x/config[\"degree2km\"] + config[\"center\"][0])\n",
    "    catalogs[\"latitude\"] = catalogs[\"y(km)\"].apply(lambda x: x/config[\"degree2km\"] + config[\"center\"][1])\n",
    "    catalogs[\"depth(m)\"] = catalogs[\"z(km)\"].apply(lambda x: x*1e3)\n",
    "    catalogs[\"event_idx\"] = range(event_idx0)\n",
    "    if config[\"use_amplitude\"]:\n",
    "        catalogs[\"covariance\"] = catalogs[\"sigma\"].apply(lambda x: f\"{x[0][0]:.3f},{x[1][1]:.3f},{x[0][1]:.3f}\")\n",
    "    else:\n",
    "        catalogs[\"covariance\"] = catalogs[\"sigma\"].apply(lambda x: f\"{x[0][0]:.3f}\")\n",
    "    with open(catalog_csv, 'w') as fp:\n",
    "        catalogs.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                        float_format=\"%.3f\",\n",
    "                        date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                        columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\", \"covariance\", \"event_idx\"])\n",
    "\n",
    "    ## add assignment to picks\n",
    "    assignments = pd.DataFrame(assignments, columns=[\"pick_idx\", \"event_idx\", \"prob_gmma\"])\n",
    "    picks = picks.join(assignments.set_index(\"pick_idx\")).fillna(-1).astype({'event_idx': int})\n",
    "    with open(picks_csv, 'w') as fp:\n",
    "        picks.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                        date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                        columns=[\"id\", \"timestamp\", \"type\", \"prob\", \"amp\", \"event_idx\", \"prob_gmma\"])\n",
    "    \n",
    "    ## upload to s3 bucket\n",
    "    try:\n",
    "        from minio import Minio\n",
    "        minioClient = Minio(s3_url,\n",
    "                      access_key='minio',\n",
    "                      secret_key='minio123',\n",
    "                      secure=secure)\n",
    "        if not minioClient.bucket_exists(bucket_name):\n",
    "            minioClient.make_bucket(bucket_name)\n",
    "        \n",
    "        with open(os.path.join(catalog_dir, f\"catalog_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "            catalogs.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                            float_format=\"%.3f\",\n",
    "                            date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                            columns=[\"time\", \"magnitude\", \"longitude\", \"latitude\", \"depth(m)\", \"covariance\", \"event_idx\"])\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/catalog_{idx[0]:04d}.csv\", os.path.join(catalog_dir, f\"catalog_{idx[0]:04d}.csv\"))\n",
    "    \n",
    "        with open(os.path.join(catalog_dir, f\"picks_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "            picks.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                            date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                            columns=[\"id\", \"timestamp\", \"type\", \"prob\", \"amp\", \"event_idx\", \"prob_gmma\"])\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/picks_{idx[0]:04d}.csv\", os.path.join(catalog_dir, f\"picks_{idx[0]:04d}.csv\"))\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f\"ERROR: can not access minio service! \\n{err}\")\n",
    "    \n",
    "    return f\"catalog_{idx[0]:04d}.csv\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_local:\n",
    "# if True:\n",
    "    catalog = gmma(0, root_dir(\"index.pkl\"), root_dir(\"config.pkl\"), root_dir(\"phasenet/picks.json\"), root_dir(\"stations.csv\"), root_dir(\"catalog.csv\"), root_dir(\"picks.csv\"),#)\n",
    "                    bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     catalog = gmma(0, \"/Users/weiqiang/Downloads/debug/index.pkl\", \"/Users/weiqiang/Downloads/debug/config.pkl\", \"/Users/weiqiang/Downloads/debug/picks.json\", \"/Users/weiqiang/Downloads/debug/stations.csv\", \n",
    "#                    \"/Users/weiqiang/Downloads/debug/catalog.csv\", \"/Users/weiqiang/Downloads/debug/picks.csv\")\n",
    "#                     bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmma_op = comp.func_to_container_op(gmma, \n",
    "                                    base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                     base_image='python:3.8',\n",
    "#                                     packages_to_install= [\n",
    "#                                          \"pandas\",\n",
    "#                                          \"numpy\",\n",
    "#                                          \"scikit-learn\",\n",
    "#                                          \"tqdm\",\n",
    "#                                          \"minio\",\n",
    "#                                          \"gmma\",\n",
    "#                                     ]\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_catalog(config_pkl: InputPath(\"pickle\"),\n",
    "                    catalog_csv: OutputPath(str),\n",
    "                    picks_csv: OutputPath(str),\n",
    "                    bucket_name:str = \"catalogs\",\n",
    "                    s3_url:str = \"minio-service:9000\", \n",
    "                    secure:bool = True):\n",
    "     \n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import os\n",
    "    import json \n",
    "    \n",
    "    from minio import Minio\n",
    "    minioClient = Minio(s3_url,\n",
    "                  access_key='minio',\n",
    "                  secret_key='minio123',\n",
    "                  secure=secure)\n",
    "    \n",
    "    with open(config_pkl, \"r\") as fp:\n",
    "        config = json.load(fp)\n",
    "    \n",
    "    objects = minioClient.list_objects(bucket_name, prefix=config[\"region\"], recursive=True)\n",
    "    \n",
    "    tmp_path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "    for obj in objects:\n",
    "        print(obj._object_name)\n",
    "        minioClient.fget_object(bucket_name, obj._object_name, tmp_path(obj._object_name.split(\"/\")[-1]))\n",
    "    \n",
    "    files_catalog = sorted(glob(tmp_path(\"catalog_*.csv\")))\n",
    "    files_picks = sorted(glob(tmp_path(\"picks_*.csv\")))\n",
    "\n",
    "    if len(files_catalog) > 0:\n",
    "        catalog_list = []\n",
    "        for f in files_catalog:\n",
    "            tmp = pd.read_csv(f, sep=\"\\t\", dtype=str)\n",
    "            tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "            catalog_list.append(tmp)        \n",
    "        combined_catalog = pd.concat(catalog_list).sort_values(by=\"time\")\n",
    "        combined_catalog.to_csv(tmp_path(\"combined_catalog.csv\"), sep=\"\\t\", index=False)\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/combined_catalog.csv\", tmp_path(\"combined_catalog.csv\"))\n",
    "        \n",
    "        pick_list = []\n",
    "        for f in files_picks:\n",
    "            tmp = pd.read_csv(f, sep=\"\\t\", dtype=str)\n",
    "            tmp[\"file_index\"] = f.rstrip(\".csv\").split(\"_\")[-1]\n",
    "            pick_list.append(tmp) \n",
    "        combined_picks = pd.concat(pick_list).sort_values(by=\"timestamp\")\n",
    "        combined_picks.to_csv(tmp_path(\"combined_picks.csv\"), sep=\"\\t\", index=False)\n",
    "        minioClient.fput_object(bucket_name, f\"{config['region']}/combined_picks.csv\", tmp_path(\"combined_picks.csv\"))\n",
    "        \n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            with open(tmp_path(\"combined_catalog.csv\"), \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    fout.write(line)\n",
    "        with open(picks_csv, \"w\") as fout:\n",
    "            with open(tmp_path(\"combined_picks.csv\"), \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    fout.write(line)\n",
    "    else:\n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            pass\n",
    "        print(\"No catalog.csv found!\")\n",
    "        with open(picks_csv, \"w\") as fout:\n",
    "            pass\n",
    "        print(\"No picks.csv found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_local\n",
    "# combine_catalog(root_dir(\"config.pkl\"), root_dir(\"combined_catalog.csv\"), root_dir(\"combined_picks.csv\"), bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_op = comp.func_to_container_op(combine_catalog, \n",
    "                                       base_image='zhuwq0/quakeflow-env:latest',\n",
    "#                                        base_image='python:3.8',\n",
    "#                                        packages_to_install= [\n",
    "#                                            \"pandas\",\n",
    "#                                            \"minio\"\n",
    "#                                        ]\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def quakeflow_pipeline(data_path:str = \"/tmp/\", \n",
    "                       num_parallel = 0,\n",
    "                       bucket_catalog:str = \"catalogs\",\n",
    "                       s3_url:str=\"minio-service:9000\", \n",
    "                       secure:bool=False):\n",
    "    \n",
    "    \n",
    "    config = config_op(num_parallel)\n",
    "\n",
    "    events = download_events_op(config.outputs[\"config_pkl\"])\n",
    "    \n",
    "    stations = download_stations_op(config.outputs[\"config_pkl\"])\n",
    "    \n",
    "\n",
    "    with kfp.dsl.ParallelFor(config.outputs[\"output\"]) as i:\n",
    "\n",
    "        vop_ = dsl.VolumeOp(name=f\"Create volume\",\n",
    "                            resource_name=f\"data-volume-{str(i)}\", \n",
    "                            size=\"100Gi\", \n",
    "                            modes=dsl.VOLUME_MODE_RWO)\n",
    "        \n",
    "        download_op_ = download_waveform_op(i, \n",
    "                                            config.outputs[\"index_pkl\"], \n",
    "                                            config.outputs[\"config_pkl\"], \n",
    "                                            config.outputs[\"datetime_pkl\"], \n",
    "                                            stations.outputs[\"station_pkl\"],\n",
    "                                            data_path = data_path\n",
    "                                           ).add_pvolumes({data_path: vop_.volume}).set_cpu_request(\"800m\").set_retry(3).set_display_name('Download Waveforms')\n",
    "        download_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "        phasenet_op_ = phasenet_op(download_op_.outputs[\"Output\"], \n",
    "                                   download_op_.outputs[\"fname_csv\"], \n",
    "                                   stations.outputs[\"station_csv\"]\n",
    "                                   ).add_pvolumes({data_path: download_op_.pvolume}).set_memory_request(\"9G\").set_display_name('PhaseNet Picking')\n",
    "        phasenet_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "\n",
    "        gmma_op_ = gmma_op(i,\n",
    "                           config.outputs[\"index_pkl\"],\n",
    "                           config.outputs[\"config_pkl\"],\n",
    "                           phasenet_op_.outputs[\"picks\"],\n",
    "                           stations.outputs[\"station_csv\"],\n",
    "                           bucket_name = f\"catalogs\",\n",
    "                           s3_url = s3_url,\n",
    "                           secure = secure\n",
    "                           ).set_cpu_request(\"800m\").set_display_name('GaMMA Association')\n",
    "#                            ).add_pvolumes({data_path: phasenet_op_.pvolume}).set_cpu_request(\"800m\").set_display_name('GaMMA Association')\n",
    "        gmma_op_.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n",
    "        \n",
    "#         vop_.delete().after(gmma_op_)\n",
    "       \n",
    "\n",
    "    combine_op_ = combine_op(config.outputs[\"config_pkl\"], bucket_name = f\"catalogs\", s3_url=s3_url, secure=secure).after(gmma_op_)\n",
    "    combine_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/weiqiang/.dotbot/cloud/quakeflow_wayne.json\"\n",
    "experiment_name = 'QuakeFlow'\n",
    "pipeline_func = quakeflow_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\"data_path\": \"/tmp\",\n",
    "             \"num_parallel\": 0,\n",
    "             \"bucket_catalog\": \"catalogs\",\n",
    "             \"s3_url\": \"minio-service:9000\",\n",
    "             \"secure\": False\n",
    "             }\n",
    "\n",
    "if not run_local:\n",
    "    client = kfp.Client(host=\"31e1ea51ab1ad4bc-dot-us-west1.pipelines.googleusercontent.com\")\n",
    "    kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "    results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                   experiment_name=experiment_name, \n",
    "                                                   run_name=run_name, \n",
    "                                                   arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quakeflow",
   "language": "python",
   "name": "quakeflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
