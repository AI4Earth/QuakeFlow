{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(index_pkl: OutputPath(\"pickle\"),\n",
    "               config_pkl: OutputPath(\"pickle\"), \n",
    "               datetime_pkl: OutputPath(\"pickle\")) -> list:\n",
    "    \n",
    "    import obspy\n",
    "    import os\n",
    "    import pickle\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    ## Location\n",
    "    pi = 3.1415926\n",
    "    degree2km = pi*6371/180\n",
    "    center = (-115.53, 32.98) #salton sea\n",
    "    horizontal_degree = 1.0\n",
    "    vertical_degree = 1.0\n",
    "\n",
    "    ## Time range\n",
    "    starttime = obspy.UTCDateTime(\"2020-10-01T00\")\n",
    "    endtime = obspy.UTCDateTime(\"2020-10-03T00\") ## not included\n",
    "\n",
    "    # seismic stations\n",
    "    network_list = \"CI\"\n",
    "    # channel_list = \"HNE,HNN,HNZ,HHE,HHN,HHZ,BHE,BHN,BHZ,EHE,EHN,EHZ\"\n",
    "    channel_list = \"HHE,HHN,HHZ\"\n",
    "\n",
    "    ####### save config ########\n",
    "    config = {}\n",
    "    config[\"center\"] = center\n",
    "    config[\"xlim_degree\"] = [center[0]-horizontal_degree/2, center[0]+horizontal_degree/2]\n",
    "    config[\"ylim_degree\"] = [center[1]-vertical_degree/2, center[1]+vertical_degree/2]\n",
    "    config[\"xlim_km\"] = [-horizontal_degree*2*degree2km/2, horizontal_degree*2*degree2km/2]\n",
    "    config[\"ylim_km\"] = [-vertical_degree*2*degree2km/2, vertical_degree*2*degree2km/2]\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime.datetime\n",
    "    config[\"endtime\"] = endtime.datetime\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"client\"] = \"SCEDC\"\n",
    "    \n",
    "    ## association\n",
    "    config[\"dims\"] = ['x(km)', 'y(km)', 'z(km)']\n",
    "    config[\"use_dbscan\"] = True\n",
    "    config[\"use_amplitude\"] = True\n",
    "    config[\"bfgs_bounds\"] = ((-horizontal_degree*2*degree2km/2-1, horizontal_degree*2*degree2km/2+1),\n",
    "                        (-vertical_degree*2*degree2km/2-1, vertical_degree*2*degree2km/2+1),\n",
    "                        (0, 21), (None, None))\n",
    "    config[\"dbscan_eps\"] = np.sqrt(horizontal_degree**2 + vertical_degree**2)*degree2km/(6.0/1.75)/2\n",
    "    config[\"dbscan_min_samples\"] = 10\n",
    "    config[\"min_picks_per_eq\"] = 6\n",
    "    config[\"oversample_factor\"] = 5\n",
    "\n",
    "    with open(config_pkl, \"wb\") as fp:\n",
    "        pickle.dump(config, fp)\n",
    "        \n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    one_hour = datetime.timedelta(hours=1)\n",
    "    starttimes = []\n",
    "    tmp_start = starttime\n",
    "    while tmp_start < endtime:\n",
    "        starttimes.append(tmp_start.datetime)\n",
    "        tmp_start += one_hour\n",
    "    \n",
    "    with open(datetime_pkl, \"wb\") as fp:\n",
    "        pickle.dump({\"starttimes\": starttimes, \"interval\": one_hour}, fp)\n",
    "        \n",
    "    num_parallel = 2\n",
    "    idx = [[] for i in range(num_parallel)]\n",
    "    for i in range(len(starttimes)):\n",
    "        idx[i - i//num_parallel*num_parallel].append(i)\n",
    "        \n",
    "    with open(index_pkl, \"wb\") as fp:\n",
    "        pickle.dump(idx, fp)\n",
    "\n",
    "    return list(range(num_parallel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "{'starttimes': [datetime.datetime(2020, 10, 1, 0, 0), datetime.datetime(2020, 10, 1, 1, 0), datetime.datetime(2020, 10, 1, 2, 0), datetime.datetime(2020, 10, 1, 3, 0), datetime.datetime(2020, 10, 1, 4, 0), datetime.datetime(2020, 10, 1, 5, 0), datetime.datetime(2020, 10, 1, 6, 0), datetime.datetime(2020, 10, 1, 7, 0), datetime.datetime(2020, 10, 1, 8, 0), datetime.datetime(2020, 10, 1, 9, 0), datetime.datetime(2020, 10, 1, 10, 0), datetime.datetime(2020, 10, 1, 11, 0), datetime.datetime(2020, 10, 1, 12, 0), datetime.datetime(2020, 10, 1, 13, 0), datetime.datetime(2020, 10, 1, 14, 0), datetime.datetime(2020, 10, 1, 15, 0), datetime.datetime(2020, 10, 1, 16, 0), datetime.datetime(2020, 10, 1, 17, 0), datetime.datetime(2020, 10, 1, 18, 0), datetime.datetime(2020, 10, 1, 19, 0), datetime.datetime(2020, 10, 1, 20, 0), datetime.datetime(2020, 10, 1, 21, 0), datetime.datetime(2020, 10, 1, 22, 0), datetime.datetime(2020, 10, 1, 23, 0), datetime.datetime(2020, 10, 2, 0, 0), datetime.datetime(2020, 10, 2, 1, 0), datetime.datetime(2020, 10, 2, 2, 0), datetime.datetime(2020, 10, 2, 3, 0), datetime.datetime(2020, 10, 2, 4, 0), datetime.datetime(2020, 10, 2, 5, 0), datetime.datetime(2020, 10, 2, 6, 0), datetime.datetime(2020, 10, 2, 7, 0), datetime.datetime(2020, 10, 2, 8, 0), datetime.datetime(2020, 10, 2, 9, 0), datetime.datetime(2020, 10, 2, 10, 0), datetime.datetime(2020, 10, 2, 11, 0), datetime.datetime(2020, 10, 2, 12, 0), datetime.datetime(2020, 10, 2, 13, 0), datetime.datetime(2020, 10, 2, 14, 0), datetime.datetime(2020, 10, 2, 15, 0), datetime.datetime(2020, 10, 2, 16, 0), datetime.datetime(2020, 10, 2, 17, 0), datetime.datetime(2020, 10, 2, 18, 0), datetime.datetime(2020, 10, 2, 19, 0), datetime.datetime(2020, 10, 2, 20, 0), datetime.datetime(2020, 10, 2, 21, 0), datetime.datetime(2020, 10, 2, 22, 0), datetime.datetime(2020, 10, 2, 23, 0)], 'interval': datetime.timedelta(seconds=3600)}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "idx = set_config(\"index.pkl\", \"config.pkl\", \"datetimes.pkl\")\n",
    "print(idx)\n",
    "with open(\"datetimes.pkl\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_op = comp.func_to_container_op(set_config, \n",
    "                                      base_image='python:3.8',\n",
    "                                      packages_to_install= [\n",
    "                                          \"numpy\",\n",
    "                                          \"obspy\"\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_events(config_pkl: InputPath(\"pickle\"),\n",
    "                    event_csv: OutputPath(str)):\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "#     import matplotlib\n",
    "#     matplotlib.use(\"agg\")\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "    with open(config_pkl, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "    \n",
    "    ####### IRIS catalog ########\n",
    "    events = Client(\"IRIS\").get_events(starttime=config[\"starttime\"],\n",
    "                                       endtime=config[\"endtime\"],\n",
    "                                       minlongitude=config[\"xlim_degree\"][0],\n",
    "                                       maxlongitude=config[\"xlim_degree\"][1],\n",
    "                                       minlatitude=config[\"ylim_degree\"][0],\n",
    "                                       maxlatitude=config[\"ylim_degree\"][1],\n",
    "                                       filename='events.xml')\n",
    "\n",
    "    events = obspy.read_events('events.xml')\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "#     events.plot('local', outfile=\"events.png\")\n",
    "\n",
    "    ####### Save catalog ########\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        catalog[\"time\"].append(event.origins[0].time.datetime)\n",
    "        catalog[\"x(km)\"].append((event.origins[0].longitude- config[\"center\"][0])*config[\"degree2km\"])\n",
    "        catalog[\"y(km)\"].append((event.origins[0].latitude - config[\"center\"][1])*config[\"degree2km\"])\n",
    "        catalog[\"z(km)\"].append(event.origins[0].depth/1e3)\n",
    "        catalog[\"mag\"].append(event.magnitudes[0].mag)\n",
    "        catalog[\"longitude\"].append(event.origins[0].longitude)\n",
    "        catalog[\"latitude\"].append(event.origins[0].latitude)\n",
    "        catalog[\"depth(m)\"].append(event.origins[0].depth)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "    catalog.to_csv(event_csv,\n",
    "                   sep=\"\\t\", index=False, float_format=\"%.3f\",\n",
    "                   date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                   columns=[\"time\", \"x(km)\", \"y(km)\", \"z(km)\", \"mag\", \"longitude\", \"latitude\", \"depth(m)\"])\n",
    "\n",
    "    ####### Plot catalog ########\n",
    "#     t = []\n",
    "#     mag = []\n",
    "#     for event in events:\n",
    "#         t.append(event.origins[0].time.datetime)\n",
    "#         mag.append(event.magnitudes[0].mag)\n",
    "#     plt.figure()\n",
    "#     plt.plot_date(t, mag)\n",
    "#     plt.gcf().autofmt_xdate()\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.title(f\"Number of events: {len(events)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"events_mag_time.png\"))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_events(\"config.pkl\", \"events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_events_op = comp.func_to_container_op(download_events, \n",
    "                                               base_image='python:3.8',\n",
    "                                               packages_to_install= [\n",
    "                                                  \"obspy\",\n",
    "                                                  \"pandas\",\n",
    "#                                                   \"matplotlib\"\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations(config_pkl: InputPath(\"pickle\"),\n",
    "                      station_csv: OutputPath(str),\n",
    "                      station_pkl: OutputPath(\"pickle\")):\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "#     import matplotlib\n",
    "#     matplotlib.use(\"agg\")\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "    with open(config_pkl, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "\n",
    "    ####### Download stations ########\n",
    "    stations = Client(\"IRIS\").get_stations(network = config[\"networks\"],\n",
    "                                           station = \"*\",\n",
    "                                           starttime=config[\"starttime\"],\n",
    "                                           endtime=config[\"endtime\"],\n",
    "                                           minlongitude=config[\"xlim_degree\"][0],\n",
    "                                           maxlongitude=config[\"xlim_degree\"][1],\n",
    "                                           minlatitude=config[\"ylim_degree\"][0],\n",
    "                                           maxlatitude=config[\"ylim_degree\"][1],\n",
    "                                           channel=config[\"channels\"],\n",
    "                                           level=\"response\",\n",
    "                                           filename=\"stations.xml\")\n",
    "\n",
    "    stations = obspy.read_inventory(\"stations.xml\")\n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "    # stations.plot('local', outfile=\"stations.png\")\n",
    "\n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                x = (chn.longitude - config[\"center\"][0])*config[\"degree2km\"]\n",
    "                y = (chn.latitude - config[\"center\"][1])*config[\"degree2km\"]\n",
    "                z = -chn.elevation / 1e3 #km\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                    station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                else:\n",
    "                    component = f\"{chn.code[-1]}\"\n",
    "                    response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                    dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict[\"x(km)\"], tmp_dict[\"y(km)\"], tmp_dict[\"z(km)\"] = x, y, z\n",
    "                    tmp_dict[\"longitude\"], tmp_dict[\"latitude\"], tmp_dict[\"elevation(m)\"] = chn.longitude, chn.latitude, chn.elevation\n",
    "                    tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"unit\"] = component, response, dtype\n",
    "                    station_locs[sid] = tmp_dict\n",
    "                    \n",
    "    station_locs = pd.DataFrame.from_dict(station_locs, orient='index')\n",
    "    station_locs.to_csv(station_csv,\n",
    "                        sep=\"\\t\", float_format=\"%.3f\",\n",
    "                        index_label=\"station\",\n",
    "                        columns=[\"x(km)\", \"y(km)\", \"z(km)\", \"latitude\", \"longitude\", \"elevation(m)\", \"unit\", \"component\", \"response\"])\n",
    "\n",
    "    with open(station_pkl, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)\n",
    "        \n",
    "#     ####### Plot stations ########\n",
    "#     plt.figure()\n",
    "#     plt.plot(station_locs[\"x(km)\"], station_locs[\"y(km)\"], \"^\", label=\"Stations\")\n",
    "# #     plt.plot(catalog[\"x(km)\"], catalog[\"y(km)\"], \"k.\", label=\"Earthquakes\")\n",
    "#     plt.xlabel(\"X (km)\")\n",
    "#     plt.ylabel(\"Y (km)\")\n",
    "#     plt.axis(\"scaled\")\n",
    "#     plt.legend()\n",
    "#     plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"stations_events.png\"))\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_stations(\"config.pkl\", \"stations.csv\", \"stations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_stations_op = comp.func_to_container_op(download_stations, \n",
    "                                                 base_image='python:3.8',\n",
    "                                                 packages_to_install= [\n",
    "                                                     \"obspy\",\n",
    "                                                     \"pandas\",\n",
    "#                                                      \"matplotlib\"\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waveform(i: int, \n",
    "                      index_pkl: InputPath(\"pickle\"),\n",
    "                      config_pkl: InputPath(\"pickle\"),\n",
    "                      datetime_pkl: InputPath(\"pickle\"),\n",
    "                      station_pkl: InputPath(\"pickle\"),\n",
    "                      fname_csv: OutputPath(str),\n",
    "                      data_path:str = \"/tmp\"\n",
    "#                       bucket_name:str = \"waveforms\",\n",
    "#                       s3_url:str = \"localhost:9000\", \n",
    "#                       secure:bool = True\n",
    "                     ) -> str:\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import time\n",
    "    \n",
    "#     from minio import Minio\n",
    "#     minioClient = Minio(s3_url,\n",
    "#                   access_key='minio',\n",
    "#                   secret_key='minio123',\n",
    "#                   secure=secure)\n",
    "    \n",
    "#     if not minioClient.bucket_exists(bucket_name):\n",
    "#         minioClient.make_bucket(bucket_name)\n",
    "\n",
    "    with open(index_pkl, \"rb\") as fp:\n",
    "        index = pickle.load(fp)\n",
    "    idx = index[i]\n",
    "    with open(config_pkl, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "    with open(datetime_pkl, \"rb\") as fp:\n",
    "        tmp = pickle.load(fp)\n",
    "        starttimes = tmp[\"starttimes\"]\n",
    "        interval = tmp[\"interval\"]\n",
    "    with open(station_pkl, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "    \n",
    "#     waveform_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "    waveform_dir = os.path.join(data_path, \"waveforms\")\n",
    "    if not os.path.exists(waveform_dir):\n",
    "        os.makedirs(waveform_dir)\n",
    "        \n",
    "    ####### Download data ########\n",
    "    client = Client(config[\"client\"])\n",
    "    fname_list = [\"fname\"]\n",
    "    for i in idx: \n",
    "        starttime = obspy.UTCDateTime(starttimes[i]) \n",
    "        endtime = starttime + interval\n",
    "        fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n",
    "#         if not overwrite:\n",
    "#         if os.path.exists(fname):\n",
    "#             print(f\"{fname} exists\")\n",
    "#             return\n",
    "        max_retry = 10\n",
    "        stream = obspy.Stream()\n",
    "        print(f\"{fname} download starts\")\n",
    "        for network in stations:\n",
    "            for station in network:\n",
    "                print(f\"********{network.code}.{station.code}********\")\n",
    "                retry = 0\n",
    "                while retry < max_retry:\n",
    "                    try:\n",
    "                        tmp = client.get_waveforms(network.code, station.code, \"*\", config[\"channels\"], starttime, endtime)\n",
    "                        stream += tmp\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(\"Error {}.{}: {}\".format(network.code, station.code,e))\n",
    "                        err = e\n",
    "                        retry += 1\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                if retry == max_retry:\n",
    "                    print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code} with error: {err}\")\n",
    "\n",
    "\n",
    "        stream.write(os.path.join(waveform_dir, fname))\n",
    "        print(f\"{fname} download succeeds\")\n",
    "#         minioClient.fput_object(bucket_name, fname, os.path.join(waveform_dir, fname))\n",
    "        fname_list.append(fname)\n",
    "    \n",
    "    with open(fname_csv, \"w\") as fp:\n",
    "        fp.write(\"\\n\".join(fname_list))\n",
    "\n",
    "    return waveform_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform_path = download_waveform(0, \"index.pkl\", \"config.pkl\", \"datetimes.pkl\", \"stations.pkl\", \"fname.csv\")\n",
    "#                                   s3_url=\"f84738f4ce13.ngrok.io\", secure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_waveform_op = comp.func_to_container_op(download_waveform, \n",
    "                                                 base_image='python:3.8',\n",
    "                                                 packages_to_install= [\n",
    "                                                     \"obspy\",\n",
    "#                                                      \"minio\"\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phasenet_op(data_path: str, \n",
    "                data_list: str, \n",
    "                stations: str):\n",
    "\n",
    "    return dsl.ContainerOp(name='PhaseNet picking',\n",
    "                           image=\"zhuwq0/phasenet:latest\",\n",
    "                           command=['python'],\n",
    "                           arguments=[\n",
    "                             'predict.py',\n",
    "                             '--model', \"model/190703-214543\",\n",
    "                             '--data_dir', data_path,\n",
    "                             '--data_list', dsl.InputArgumentPath(data_list),\n",
    "                             '--stations', dsl.InputArgumentPath(stations),\n",
    "#                              '--result_dir', \"results\",\n",
    "                             '--input_mseed',\n",
    "                             '--amplitude'\n",
    "                             ],\n",
    "                           file_outputs = {\"picks\": \"/opt/results/picks.json\"}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python PhaseNet/phasenet/predict.py --model=PhaseNet/model/190703-214543 --data_list=fname.csv --data_dir=waveforms --stations=stations.csv --input_mseed --amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmma(i: int,\n",
    "         index_pkl: InputPath(\"pickle\"),\n",
    "         config_pkl: InputPath(\"pickle\"),\n",
    "         pick_json: InputPath(\"json\"),\n",
    "         station_csv: InputPath(str),\n",
    "         catalog_csv: OutputPath(str),\n",
    "         bucket_name:str = \"catalogs\",\n",
    "         s3_url:str = \"localhost:9000\", \n",
    "         secure:bool = True) -> str:\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from gmma import mixture\n",
    "    import numpy as np\n",
    "    from sklearn.cluster import DBSCAN \n",
    "    from datetime import datetime, timedelta\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    \n",
    "    from minio import Minio\n",
    "    minioClient = Minio(s3_url,\n",
    "                  access_key='minio',\n",
    "                  secret_key='minio123',\n",
    "                  secure=secure)\n",
    "    if not minioClient.bucket_exists(bucket_name):\n",
    "        minioClient.make_bucket(bucket_name)\n",
    "\n",
    "    \n",
    "    with open(index_pkl, \"rb\") as fp:\n",
    "        index = pickle.load(fp)\n",
    "    idx = index[i]\n",
    "    \n",
    "    with open(config_pkl, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "        \n",
    "    catalog_dir = os.path.join(\"/tmp/\", bucket_name)\n",
    "    if not os.path.exists(catalog_dir):\n",
    "        os.makedirs(catalog_dir)\n",
    "    \n",
    "    to_seconds = lambda t: datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\").timestamp()\n",
    "    from_seconds = lambda t: [datetime.fromtimestamp(x).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] for x in t]\n",
    "\n",
    "    def convert_picks(picks, stations, config):\n",
    "        data, locs, phase_type, phase_weight = ([],[],[],[])\n",
    "        for pick in picks:\n",
    "            data.append([to_seconds(pick[\"timestamp\"]), np.log10(pick[\"amp\"]*1e2)]) #cm/s\n",
    "            locs.append(stations.loc[pick[\"id\"]][config[\"dims\"]].values.astype(\"float\"))\n",
    "            phase_type.append(pick[\"type\"].lower())\n",
    "            phase_weight.append(pick[\"prob\"])\n",
    "        data = np.array(data)\n",
    "        locs = np.array(locs)\n",
    "        phase_weight = np.array(phase_weight)[:,np.newaxis]\n",
    "        return data, locs, phase_type, phase_weight\n",
    "    \n",
    "    def association(data, locs, phase_type, phase_weight, num_sta, config):\n",
    "        \n",
    "        db = DBSCAN(eps=config[\"dbscan_eps\"], min_samples=config[\"dbscan_min_samples\"]).fit(data[:,0:1])\n",
    "        labels = db.labels_\n",
    "        unique_labels = set(labels)\n",
    "        events = []\n",
    "        for k in unique_labels:\n",
    "            if k == -1:\n",
    "                continue\n",
    "\n",
    "            class_mask = (labels == k)\n",
    "            data_ = data[class_mask]\n",
    "            locs_ = locs[class_mask]\n",
    "            phase_type_ = np.array(phase_type)[class_mask]\n",
    "            phase_weight_ = phase_weight[class_mask]\n",
    "\n",
    "            num_event_ = min(max(int(len(data_)/num_sta*config[\"oversample_factor\"]), 1), len(data_))\n",
    "            t0 = data_[:,0].min()\n",
    "            t_range = max(data_[:,0].max() - data_[:,0].min(), 1)\n",
    "            centers_init = np.vstack([np.ones(num_event_)*np.mean(stations[\"x(km)\"]),\n",
    "                                      np.ones(num_event_)*np.mean(stations[\"y(km)\"]),\n",
    "                                      np.zeros(num_event_),\n",
    "                                      np.linspace(data_[:,0].min()-0.1*t_range, data_[:,0].max()+0.1*t_range, num_event_)]).T # n_eve, n_dim(x, y, z) + 1(t)\n",
    "\n",
    "\n",
    "            if config[\"use_amplitude\"]:\n",
    "                covariance_prior = np.array([[1,0],[0,1]]) * 3\n",
    "            else:\n",
    "                covariance_prior = np.array([[1]])\n",
    "                data = data[:,0:1]\n",
    "            gmm = mixture.BayesianGaussianMixture(n_components=num_event_, \n",
    "                                                  weight_concentration_prior=1000/num_event_,\n",
    "                                                  mean_precision_prior = 0.3/t_range,\n",
    "                                                  covariance_prior = covariance_prior,\n",
    "                                                  init_params=\"centers\",\n",
    "                                                  centers_init=centers_init, \n",
    "                                                  station_locs=locs_, \n",
    "                                                  phase_type=phase_type_, \n",
    "                                                  phase_weight=phase_weight_,\n",
    "                                                  loss_type=\"l1\",\n",
    "                                                  bounds=config[\"bfgs_bounds\"],\n",
    "                                                  max_covar=10.0,\n",
    "                                                  reg_covar=0.1,\n",
    "                                                  ).fit(data_) \n",
    "\n",
    "            pred = gmm.predict(data_) \n",
    "            prob = gmm.predict_proba(data_)\n",
    "            prob_eq = prob.mean(axis=0)\n",
    "            prob_data = prob[range(len(data_)), pred]\n",
    "            score_data = gmm.score_samples(data_)\n",
    "\n",
    "            idx = np.array([True if len(data_[pred==i, 0]) >= config[\"min_picks_per_eq\"] else False for i in range(len(prob_eq))]) #& (prob_eq > 1/num_event) #& (std_eq[:, 0,0] < 40)\n",
    "            eq_idx = np.arange(len(idx))[idx]\n",
    "\n",
    "            time = from_seconds(gmm.centers_[idx, len(config[\"dims\"])])\n",
    "            loc = gmm.centers_[idx, :len(config[\"dims\"])]\n",
    "            if config[\"use_amplitude\"]:\n",
    "                mag = gmm.centers_[idx, len(config[\"dims\"])+1]\n",
    "            std_eq = gmm.covariances_[idx,...]\n",
    "\n",
    "            for i in range(len(time)):\n",
    "                tmp = {\"time\": time[i],\n",
    "                       \"magnitude\": mag[i],\n",
    "                       \"std\": std_eq[i].tolist()}\n",
    "                for j, k in enumerate(config[\"dims\"]):\n",
    "                    tmp[k] = loc[i][j]\n",
    "                events.append(tmp)\n",
    "\n",
    "        return events\n",
    "    \n",
    "    with open(pick_json, \"r\") as fp:\n",
    "        picks = json.load(fp)\n",
    "    stations = pd.read_csv(station_csv, delimiter=\"\\t\", index_col=\"station\")\n",
    "    data, locs, phase_type, phase_weight = convert_picks(picks, stations, config)\n",
    "    catalog = association(data, locs, phase_type, phase_weight, len(stations), config)\n",
    "    catalog = pd.DataFrame(catalog, columns=[\"time\"]+config[\"dims\"]+[\"magnitude\", \"std\"])\n",
    "    \n",
    "    with open(catalog_csv, 'w') as fp:\n",
    "        catalog.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                       float_format=\"%.3f\",\n",
    "                       date_format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "    \n",
    "    with open(os.path.join(catalog_dir, f\"catalog_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "        catalog.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                       float_format=\"%.3f\",\n",
    "                       date_format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "        \n",
    "    minioClient.fput_object(bucket_name, f\"catalog_{i:04d}.csv\", os.path.join(catalog_dir, f\"catalog_{i:04d}.csv\"))\n",
    "    \n",
    "    return f\"catalog_{i:04d}.csv\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catalog_path = gmma(1, \"index.pkl\", \"config.pkl\", \"./results/picks.json\", \"stations.csv\", \"catalog.csv\", #)\n",
    "#                     bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmma_op = comp.func_to_container_op(gmma, \n",
    "                                    base_image='python:3.8',\n",
    "                                    packages_to_install= [\n",
    "                                         \"pandas\",\n",
    "                                         \"numpy\",\n",
    "                                         \"scikit-learn\",\n",
    "                                         \"minio\",\n",
    "                                         \"gmma\"\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_catalog(catalog_csv: OutputPath(str),\n",
    "                    bucket_name:str = \"catalogs\",\n",
    "                    s3_url:str = \"localhost:9000\", \n",
    "                    secure:bool = True):\n",
    "     \n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import os\n",
    "    \n",
    "    from minio import Minio\n",
    "    minioClient = Minio(s3_url,\n",
    "                  access_key='minio',\n",
    "                  secret_key='minio123',\n",
    "                  secure=secure)\n",
    "    objects = minioClient.list_objects(bucket_name, recursive=True)\n",
    "    \n",
    "    tmp_path = lambda x: os.path.join(\"/tmp/\", x)\n",
    "    for obj in objects:\n",
    "        minioClient.fget_object(bucket_name, obj._object_name, tmp_path(obj._object_name))\n",
    "    \n",
    "    files_catalog = sorted(glob(tmp_path(\"catalog_*.csv\")))\n",
    "\n",
    "    if len(files_catalog) > 0:\n",
    "        combined_catalog = pd.concat([pd.read_csv(f, sep=\"\\t\", dtype=str) for f in files_catalog]).sort_values(by=\"time\")\n",
    "        combined_catalog.to_csv(tmp_path(\"combined_catalog.csv\"), sep=\"\\t\", index=False)\n",
    "        minioClient.fput_object(bucket_name, f\"combined_catalog.csv\", tmp_path(\"combined_catalog.csv\"))\n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            with open(tmp_path(\"combined_catalog.csv\"), \"r\") as fin:\n",
    "                for line in fin:\n",
    "                    fout.write(line)\n",
    "    else:\n",
    "        with open(catalog_csv, \"w\") as fout:\n",
    "            pass\n",
    "        print(\"No events.csv found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_catalog(\"catalog.csv\", bucket_name=\"catalogs\", s3_url=\"localhost:9000\", secure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_op = comp.func_to_container_op(combine_catalog, \n",
    "                                       base_image='python:3.8',\n",
    "                                       packages_to_install= [\n",
    "                                           \"pandas\",\n",
    "                                           \"minio\"\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def quakeflow_pipeline(data_path:str = \"/tmp/\", \n",
    "                       bucket_catalog:str = \"catalogs\",\n",
    "                       s3_url:str=\"localhost:9000\", \n",
    "                       secure:bool=False):\n",
    "    \n",
    "#     s3_url = \"10.3.252.218\"\n",
    "#     s3_url = \"127.0.0.1\"\n",
    "#     pvop = dsl.VolumeOp(name=\"Create_volume\",\n",
    "#                        resource_name=\"data-volume\", \n",
    "#                        size=\"10Gi\", \n",
    "#                        modes=dsl.VOLUME_MODE_RWO).volume\n",
    "#     pvop = dsl.PipelineVolume(pvc=\"quakeflow-smw46-data-volume\")\n",
    "    \n",
    "#     def set_config(index_pkl: OutputPath(\"pickle\"),\n",
    "#                    config_pkl: OutputPath(\"pickle\"), \n",
    "#                    datetime_pkl: OutputPath(\"pickle\")) -> list:\n",
    "    \n",
    "    config = config_op()\n",
    "        \n",
    "#     def download_events(config_pkl: InputPath(\"pickle\"),\n",
    "#                         event_csv: OutputPath(str)):\n",
    "    events = download_events_op(config.outputs[\"config_pkl\"])\n",
    "    \n",
    "#     def download_stations(config_pkl: InputPath(\"pickle\"),\n",
    "#                           station_csv: OutputPath(str),\n",
    "#                           station_pkl: OutputPath(\"pkl\")):\n",
    "    stations = download_stations_op(config.outputs[\"config_pkl\"])\n",
    "\n",
    "    with kfp.dsl.ParallelFor(config.outputs[\"output\"]) as i:\n",
    "        \n",
    "        vop_ = dsl.VolumeOp(name=\"Create volume\",\n",
    "                            resource_name=\"data_volume\", \n",
    "                            size=\"10Gi\", \n",
    "                            modes=dsl.VOLUME_MODE_RWO)\n",
    "        \n",
    "#         def download_waveform(i: int, \n",
    "#                               index_pkl: InputPath(\"pickle\"),\n",
    "#                               config_pkl: InputPath(\"pickle\"),\n",
    "#                               datetime_pkl: InputPath(\"pickle\"),\n",
    "#                               station_pkl: InputPath(\"pickle\"),\n",
    "#                               fname_csv: OutputPath(str),\n",
    "#                               data_path:str = \"/tmp\"\n",
    "#         #                       bucket_name:str = \"waveforms\",\n",
    "#         #                       s3_url:str = \"localhost:9000\", \n",
    "#         #                       secure:bool = True\n",
    "#                              ):\n",
    "        download_op_ = download_waveform_op(i, \n",
    "                                            config.outputs[\"index_pkl\"], \n",
    "                                            config.outputs[\"config_pkl\"], \n",
    "                                            config.outputs[\"datetime_pkl\"], \n",
    "                                            stations.outputs[\"station_pkl\"],\n",
    "                                            data_path = data_path\n",
    "                                           ).add_pvolumes({data_path: vop_.volume})\n",
    "#         download_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "        \n",
    "#         def phasenet_op(data_path: str, \n",
    "#                         data_list: str, \n",
    "#                         stations: str):\n",
    "        phasenet_op_ = phasenet_op(download_op_.outputs[\"Output\"], \n",
    "                                   download_op_.outputs[\"fname_csv\"], \n",
    "                                   stations.outputs[\"station_csv\"]\n",
    "                                   ).add_pvolumes({data_path: download_op_.pvolume})\n",
    "#         phasenet_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "#         def gmma(i,\n",
    "#                  index_pkl: InputPath(\"pickle\"),\n",
    "#                  config_pkl: InputPath(\"pickle\"),\n",
    "#                  pick_json: InputPath(\"json\"),\n",
    "#                  station_csv: InputPath(str),\n",
    "#                  catalog_csv: OutputPath(str),\n",
    "#                  bucket_name:str = \"catalogs\",\n",
    "#                  s3_url:str = \"localhost:9000\", \n",
    "#                  secure:bool = True) -> str:\n",
    "        gmma_op_ = gmma_op(i,\n",
    "                           config.outputs[\"index_pkl\"],\n",
    "                           config.outputs[\"config_pkl\"],\n",
    "                           phasenet_op_.outputs[\"picks\"],\n",
    "                           stations.outputs[\"station_csv\"],\n",
    "                           bucket_name = \"catalogs\",\n",
    "                           s3_url = s3_url,\n",
    "                           secure = secure\n",
    "                           ).add_pvolumes({data_path: phasenet_op_.pvolume})\n",
    "\n",
    "#     def combine_catalog(catalog_csv: OutputPath(str),\n",
    "#                         bucket_name:str = \"catalogs\",\n",
    "#                         s3_url:str = \"localhost:9000\", \n",
    "#                         secure:bool = True):\n",
    "    combine_op_ = combine_op(bucket_name = \"catalogs\", s3_url=s3_url, secure=secure).after(gmma_op_)\n",
    "    combine_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='https://97745bc77884.ngrok.io/')\n",
    "# client = kfp.Client(host='127.0.0.1:8080')\n",
    "# client = kfp.Client(host='553ab00ece5a86e5-dot-us-west1.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://97745bc77884.ngrok.io//#/experiments/details/aeefa653-9ac2-42e7-910f-b5340336ada9\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://97745bc77884.ngrok.io//#/runs/details/b370a5f7-2482-47e5-9fc6-d625e06503f7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'QuakeFlow'\n",
    "pipeline_func = quakeflow_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\"data_path\": \"/tmp/\",\n",
    "             \"bucket_catalog\": \"catalogs\",\n",
    "#              \"s3_url\": \"localhost:9000\",\n",
    "#              \"secure\": False\n",
    "#              \"s3_url\": \"10.111.90.219:9000\",\n",
    "             \"s3_url\": \"10.97.200.84:9000\",\n",
    "             \"secure\": False\n",
    "             }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
