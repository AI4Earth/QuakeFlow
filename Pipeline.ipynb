{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(data_path: str, \n",
    "               config_file:OutputPath(\"pickle\"), \n",
    "               datetime_file:OutputPath(\"pickle\")) -> list:\n",
    "    \n",
    "    import obspy\n",
    "    import os\n",
    "    import pickle\n",
    "    import datetime\n",
    "    \n",
    "    # Location\n",
    "    pi = 3.1415926\n",
    "    degree2km = pi*6371/180\n",
    "    center = (-115.53, 32.98) #salton sea\n",
    "    horizontal_degree = 0.5\n",
    "    vertical_degree = 0.5\n",
    "    zero_anchor = (center[0]-horizontal_degree, center[1]-vertical_degree)\n",
    "\n",
    "    # Time\n",
    "    starttime = obspy.UTCDateTime(\"2020-10-01\")\n",
    "    endtime = obspy.UTCDateTime(\"2020-10-03\") ## not included\n",
    "\n",
    "    # seismic stations\n",
    "    network_list = \"CI\"\n",
    "    # channel_list = \"HNE,HNN,HNZ,HHE,HHN,HHZ,BHE,BHN,BHZ,EHE,EHN,EHZ\"\n",
    "    channel_list = \"HHE,HHN,HHZ\"\n",
    "    \n",
    "#     print(data_path)\n",
    "#     if not os.path.exists(data_path):\n",
    "#         os.makedirs(data_path)\n",
    "#     if not os.path.exists(os.path.join(data_path, \"outputs/Output/\")):\n",
    "#         os.makedirs(os.path.join(data_path, \"outputs/Output/\"))\n",
    "#         open(os.path.join(data_path, \"outputs/Output/data\"), \"a\").close()\n",
    "    \n",
    "    ####### save config ########\n",
    "    config = {}\n",
    "    config[\"center\"] = center\n",
    "    config[\"horizontal_degree\"] = horizontal_degree\n",
    "    config[\"vertical_degree\"] = vertical_degree\n",
    "    config[\"zero_anchor\"] = zero_anchor\n",
    "    config[\"xlim\"] = [0, horizontal_degree*2*degree2km]\n",
    "    config[\"ylim\"] = [0, vertical_degree*2*degree2km]\n",
    "    config[\"anchor\"] = zero_anchor\n",
    "    config[\"degree2km\"] = degree2km\n",
    "    config[\"starttime\"] = starttime\n",
    "    config[\"endtime\"] = endtime\n",
    "    config[\"networks\"] = network_list\n",
    "    config[\"channels\"] = channel_list\n",
    "    config[\"network_list\"] = network_list\n",
    "    config[\"channel_list\"] = channel_list\n",
    "#     config[\"station_response\"] = stations\n",
    "#     with open(os.path.join(data_path, \"config.pkl\"), \"wb\") as fp:\n",
    "#         pickle.dump(config, fp)\n",
    "    with open(config_file, \"wb\") as fp:\n",
    "        pickle.dump(config, fp)\n",
    "        \n",
    "    one_day = datetime.timedelta(days=1)\n",
    "    one_hour = datetime.timedelta(hours=1)\n",
    "    starttimes = []\n",
    "    tmp_start = starttime\n",
    "    while tmp_start < endtime:\n",
    "        starttimes.append(tmp_start)\n",
    "        tmp_start += one_hour\n",
    "    \n",
    "#     with open(os.path.join(data_path, \"datetimes.pkl\"), \"wb\") as fp:\n",
    "#         pickle.dump({\"starttimes\": starttimes, \"interval\": one_hour}, fp)\n",
    "    with open(datetime_file, \"wb\") as fp:\n",
    "        pickle.dump({\"starttimes\": starttimes, \"interval\": one_hour}, fp)\n",
    "        \n",
    "    num_parallel = 2\n",
    "    idx = [[] for i in range(num_parallel)]\n",
    "    for i in range(len(starttimes)):\n",
    "        idx[i - i//num_parallel*num_parallel].append(i)\n",
    "\n",
    "    return tuple(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46], [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47])\n"
     ]
    }
   ],
   "source": [
    "idx = set_config(\"\", \"config.pkl\", \"datetimes.pkl\")\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UTCDateTime(2020, 10, 1, 0, 0), UTCDateTime(2020, 10, 1, 1, 0)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"./test/datetimes.pkl\", \"rb\") as fp:\n",
    "    data = pickle.load(fp)\n",
    "print(data[\"starttimes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_op = comp.func_to_container_op(set_config, \n",
    "                                      base_image='python:3.8',\n",
    "                                      packages_to_install= [\n",
    "                                          \"obspy\"\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_events(data_path, \n",
    "                    config_file: InputPath(\"pickle\"),\n",
    "                    event_file: OutputPath(str)):\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    \n",
    "#     with open(os.path.join(data_path, \"config.pkl\"), \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_file, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "    \n",
    "    ####### IRIS catalog ########\n",
    "    events = Client(\"IRIS\").get_events(starttime=config[\"starttime\"],\n",
    "                                       endtime=config[\"endtime\"],\n",
    "                                       minlatitude=config[\"center\"][1]-config[\"vertical_degree\"],\n",
    "                                       maxlatitude=config[\"center\"][1]+config[\"vertical_degree\"],\n",
    "                                       minlongitude=config[\"center\"][0]-config[\"horizontal_degree\"],\n",
    "                                       maxlongitude=config[\"center\"][0]+config[\"horizontal_degree\"],\n",
    "                                       filename=os.path.join(data_path, 'events.xml'))\n",
    "\n",
    "    events = obspy.read_events(os.path.join(data_path, 'events.xml'))\n",
    "    print(f\"Number of events: {len(events)}\")\n",
    "    # events.plot('local', outfile=\"events.png\")\n",
    "\n",
    "    ####### Save catalog ########\n",
    "    catalog = defaultdict(list)\n",
    "    for event in events:\n",
    "        catalog[\"time\"].append(event.origins[0].time.datetime)\n",
    "        catalog[\"x(km)\"].append((event.origins[0].longitude- config[\"zero_anchor\"][0])*config[\"degree2km\"])\n",
    "        catalog[\"y(km)\"].append((event.origins[0].latitude - config[\"zero_anchor\"][1])*config[\"degree2km\"])\n",
    "        catalog[\"z(km)\"].append(event.origins[0].depth/1e3)\n",
    "        catalog[\"mag\"].append(event.magnitudes[0].mag)\n",
    "        catalog[\"lng\"].append(event.origins[0].longitude)\n",
    "        catalog[\"lat\"].append(event.origins[0].latitude)\n",
    "        catalog[\"depth(m)\"].append(event.origins[0].depth)\n",
    "    catalog = pd.DataFrame.from_dict(catalog).sort_values([\"time\"])\n",
    "#     catalog.to_csv(\"events.csv\",\n",
    "    catalog.to_csv(event_file,\n",
    "                    sep=\"\\t\", index=False, float_format=\"%.3f\",\n",
    "                    date_format='%Y-%m-%dT%H:%M:%S.%f',\n",
    "                    columns=[\"time\", \"x(km)\", \"y(km)\", \"z(km)\", \"mag\", \"lng\", \"lat\", \"depth(m)\"])\n",
    "\n",
    "    ####### Plot catalog ########\n",
    "#     t = []\n",
    "#     mag = []\n",
    "#     for event in events:\n",
    "#         t.append(event.origins[0].time.datetime)\n",
    "#         mag.append(event.magnitudes[0].mag)\n",
    "#     plt.figure()\n",
    "#     plt.plot_date(t, mag)\n",
    "#     plt.gcf().autofmt_xdate()\n",
    "#     plt.ylabel(\"Magnitude\")\n",
    "#     plt.title(f\"Number of events: {len(events)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"events_mag_time.png\"))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 1062\n"
     ]
    }
   ],
   "source": [
    "download_events(\"\", \"config.pkl\", \"events.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_events_op = comp.func_to_container_op(download_events, \n",
    "                                              base_image='python:3.8',\n",
    "                                              packages_to_install= [\n",
    "                                                  \"obspy\",\n",
    "                                                  \"pandas\",\n",
    "                                                  \"matplotlib\"\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stations(data_path: str, \n",
    "                      config_file: InputPath(\"pickle\"),\n",
    "                      station_list: OutputPath(str),\n",
    "                      station_file: OutputPath(\"pickle\")):\n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import defaultdict\n",
    "    import pandas as pd\n",
    "    \n",
    "#     with open(os.path.join(data_path, \"config.pkl\"), \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_file, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "\n",
    "    ####### Download stations ########\n",
    "    stations = Client(\"IRIS\").get_stations(network = config[\"network_list\"],\n",
    "                                           station = \"*\",\n",
    "                                           starttime=config[\"starttime\"],\n",
    "                                           endtime=config[\"endtime\"],\n",
    "                                           minlatitude=config[\"center\"][1]-config[\"vertical_degree\"],\n",
    "                                           maxlatitude=config[\"center\"][1]+config[\"vertical_degree\"],\n",
    "                                           minlongitude=config[\"center\"][0]-config[\"horizontal_degree\"],\n",
    "                                           maxlongitude=config[\"center\"][0]+config[\"horizontal_degree\"],\n",
    "                                           channel=config[\"channel_list\"],\n",
    "                                           level=\"response\",\n",
    "                                           filename=os.path.join(data_path, 'stations.xml'))\n",
    "\n",
    "    stations = obspy.read_inventory(os.path.join(data_path, 'stations.xml'))\n",
    "    print(\"Number of stations: {}\".format(sum([len(x) for x in stations])))\n",
    "    # stations.plot('local', outfile=\"stations.png\")\n",
    "\n",
    "    ####### Save stations ########\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in stations:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                x = (chn.longitude - config[\"zero_anchor\"][0])*config[\"degree2km\"]\n",
    "                y = (chn.latitude - config[\"zero_anchor\"][1])*config[\"degree2km\"]\n",
    "                z = -chn.elevation / 1e3 #km\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\"\n",
    "                if sid in station_locs:\n",
    "                    station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                    station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                else:\n",
    "                    component = f\"{chn.code[-1]}\"\n",
    "                    response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                    dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict[\"x(km)\"], tmp_dict[\"y(km)\"], tmp_dict[\"z(km)\"] = x, y, z\n",
    "                    tmp_dict[\"lng\"], tmp_dict[\"lat\"], tmp_dict[\"elv(m)\"] = chn.longitude, chn.latitude, chn.elevation\n",
    "                    tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"type\"] = component, response, dtype\n",
    "                    station_locs[sid] = tmp_dict\n",
    "    station_locs = pd.DataFrame.from_dict(station_locs, orient='index')\n",
    "#     station_locs.to_csv(\"stations.csv\",\n",
    "#                     sep=\"\\t\", float_format=\"%.3f\",\n",
    "#                     index_label=\"station\",\n",
    "#                     columns=[\"x(km)\", \"y(km)\", \"z(km)\", \"lat\", \"lng\", \"elv(m)\", \"type\", \"component\", \"response\"])\n",
    "    station_locs.to_csv(station_list,\n",
    "                        sep=\"\\t\", float_format=\"%.3f\",\n",
    "                        index_label=\"station\",\n",
    "                        columns=[\"x(km)\", \"y(km)\", \"z(km)\", \"lat\", \"lng\", \"elv(m)\", \"type\", \"component\", \"response\"])\n",
    "\n",
    "#     ####### Plot stations ########\n",
    "#     plt.figure()\n",
    "#     plt.plot(station_locs[\"x(km)\"], station_locs[\"y(km)\"], \"^\", label=\"Stations\")\n",
    "# #     plt.plot(catalog[\"x(km)\"], catalog[\"y(km)\"], \"k.\", label=\"Earthquakes\")\n",
    "#     plt.xlabel(\"X (km)\")\n",
    "#     plt.ylabel(\"Y (km)\")\n",
    "#     plt.axis(\"scaled\")\n",
    "#     plt.legend()\n",
    "#     plt.title(f\"Number of stations: {len(station_locs)}\")\n",
    "#     plt.savefig(os.path.join(data_path, \"stations_events.png\"))\n",
    "#     # plt.show()\n",
    "    \n",
    "#     config[\"station_list\"] = stations\n",
    "#     with open(os.path.join(data_path, \"config.pkl\"), \"wb\") as fp:\n",
    "#         pickle.dump(config, fp)\n",
    "    with open(station_file, \"wb\") as fp:\n",
    "        pickle.dump(stations, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations: 16\n"
     ]
    }
   ],
   "source": [
    "download_stations(\"\", \"config.pkl\", \"stations.csv\", \"stations.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_stations_op = comp.func_to_container_op(download_stations, \n",
    "                                                 base_image='python:3.8',\n",
    "                                                 packages_to_install= [\n",
    "                                                     \"obspy\",\n",
    "                                                     \"pandas\",\n",
    "                                                     \"matplotlib\"\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_waveform(data_path: str, \n",
    "                      idx: list, \n",
    "                      config_file: InputPath(\"pickle\"),\n",
    "                      datetime_file: InputPath(\"pickle\"),\n",
    "                      station_file: InputPath(\"pickle\"),\n",
    "                      fname_list: OutputPath(str),\n",
    "                      s3_url:str=\"localhost:9000\", \n",
    "                      secure:bool=False) -> str:\n",
    "    \n",
    "    import pickle, os\n",
    "    import obspy\n",
    "    from obspy.clients.fdsn import Client\n",
    "#     from minio import Minio\n",
    "#     from minio.error import (ResponseError, BucketAlreadyOwnedByYou,\n",
    "#                              BucketAlreadyExists)\n",
    "#     minioClient = Minio(f'{s3_url}',\n",
    "#                   access_key='quakeflow',\n",
    "#                   secret_key='quakeflow',\n",
    "#                   secure=secure)\n",
    "    \n",
    "#     with open(os.path.join(data_path, \"config.pkl\"), \"rb\") as fp:\n",
    "#         config = pickle.load(fp)\n",
    "    with open(config_file, \"rb\") as fp:\n",
    "        config = pickle.load(fp)\n",
    "#     with open(os.path.join(data_path, \"datetimes.pkl\"), \"rb\") as fp:\n",
    "    with open(datetime_file, \"rb\") as fp:\n",
    "        tmp = pickle.load(fp)\n",
    "        starttimes = tmp[\"starttimes\"]\n",
    "        interval = tmp[\"interval\"]\n",
    "    with open(station_file, \"rb\") as fp:\n",
    "        stations = pickle.load(fp)\n",
    "    \n",
    "    waveform_dir = os.path.join(data_path, \"waveforms\")\n",
    "    ####### Download data ########\n",
    "    client = Client(\"SCEDC\")\n",
    "#     def download_all_stations(starttime, interval, overwrite=False):\n",
    "    fp = open(fname_list, \"w\")\n",
    "    fp.write(\"fname\\n\")\n",
    "    for i in idx: \n",
    "        starttime = starttimes[i] \n",
    "        endtime = starttime + interval\n",
    "#         fname = os.path.join(output_dir, \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H\")))\n",
    "        fname = \"{}.mseed\".format(starttime.datetime.strftime(\"%Y-%m-%dT%H\"))\n",
    "#         if not overwrite:\n",
    "#         if os.path.exists(fname):\n",
    "#             print(f\"{fname} exists\")\n",
    "#             return\n",
    "\n",
    "        max_retry = 3\n",
    "        stream = obspy.Stream()\n",
    "        print(f\"{fname} download starts\")\n",
    "#         for network in config[\"station_list\"]:\n",
    "        for network in stations:\n",
    "            for station in network:\n",
    "                # logger.info(f\"********{network.code}.{station.code}********\")\n",
    "                retry = 0\n",
    "                while retry < max_retry:\n",
    "                    try:\n",
    "                        tmp = client.get_waveforms(network.code, station.code, \"*\", config[\"channel_list\"], starttime, endtime)\n",
    "                        stream += tmp\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                    #           logger.warning(\"Error {}.{}: {}\".format(network.code, station.code,e))\n",
    "                        err = e\n",
    "                        retry += 1\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                if retry == max_retry:\n",
    "                    print(f\"{fname}: MAX {max_retry} retries reached : {network.code}.{station.code} with error: {err}\")\n",
    "\n",
    "#         stream.write(os.path.join(\"/tmp/\", fname))\n",
    "        if not os.path.exists(waveform_dir):\n",
    "            os.makedirs(waveform_dir)\n",
    "        stream.write(os.path.join(waveform_dir, fname))\n",
    "        print(f\"{fname} download succeeds\")\n",
    "        fp.write(f\"{fname}\\n\")\n",
    "\n",
    "#         # Make a bucket with the make_bucket API call.\n",
    "#         try:\n",
    "#             minioClient.make_bucket(\"waveforms\")\n",
    "#         except BucketAlreadyOwnedByYou as err:\n",
    "#             pass\n",
    "#         except BucketAlreadyExists as err:\n",
    "#             pass\n",
    "#         except ResponseError as err:\n",
    "#             raise\n",
    "\n",
    "#         # Put an object 'pumaserver_debug.log' with contents from 'pumaserver_debug.log'.\n",
    "#         try:\n",
    "# #             minioClient.fput_object('waveforms', fname, os.path.join(\"/tmp/\", fname))\n",
    "#             minioClient.fput_object('waveforms', fname, os.path.join(data_path, \"waveforms\", fname))\n",
    "#         except ResponseError as err:\n",
    "#             print(err)\n",
    "    print(os.listdir(waveform_dir))\n",
    "    fp.close()\n",
    "\n",
    "    return waveform_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01T01.mseed download starts\n",
      "2020-10-01T01.mseed download succeeds\n",
      "['2020-10-01T01.mseed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./test/waveforms'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download_waveform(\"./test\", idx=[1], s3_url=\"18663dd7908c.ngrok.io\", secure=True)\n",
    "download_waveform(\"./test\", [1], \"config.pkl\", \"datetimes.pkl\", \"stations.pkl\", \"fname.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_waveform_op = comp.func_to_container_op(download_waveform, \n",
    "                                                 base_image='python:3.8',\n",
    "                                                 packages_to_install= [\n",
    "                                                     \"obspy\",\n",
    "#                                                      \"minio\"\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phasenet_op(data_dir: str, \n",
    "                data_list: str, \n",
    "                stations: str):\n",
    "\n",
    "    return dsl.ContainerOp(name='PhaseNet picking',\n",
    "                           image=\"zhuwq0/phasenet:0.2\",\n",
    "                           command=['python'],\n",
    "                           arguments=[\n",
    "                             'predict.py',\n",
    "                             '--model', \"model/190703-214543\",\n",
    "                             '--data_dir', data_dir,\n",
    "                             '--data_list', dsl.InputArgumentPath(data_list),\n",
    "                             '--stations', dsl.InputArgumentPath(stations),\n",
    "#                              '--result_dir', \"results\",\n",
    "                             '--input_mseed'\n",
    "                             ],\n",
    "                           file_outputs = {\"picks\": \"/opt/results/picks.csv\"}\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmma(data_path: str,\n",
    "         idx: list,\n",
    "         pick_list: InputPath(str),\n",
    "         station_list: InputPath(str),\n",
    "         catalog_path: OutputPath(str)):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime, timedelta\n",
    "    from gmma import mixture\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    def read_picks(fpick):\n",
    "        with open(fpick, \"r\") as fp:\n",
    "            picks = pd.read_csv(fp)\n",
    "        picks[\"time\"] = picks[\"fname\"].map(lambda x: x.split(\".\")[0])\n",
    "        picks[\"station\"] = picks[\"fname\"].map(lambda x: \".\".join(x.split(\".\")[1:]))\n",
    "        picks[\"itp\"] = picks[\"itp\"].map(lambda x: [round(float(i)*dt, 2) for i in x.strip(\"[]\").split(\" \") if i != \"\"])\n",
    "        picks[\"its\"] = picks[\"its\"].map(lambda x: [round(float(i)*dt, 2) for i in x.strip(\"[]\").split(\" \") if i != \"\"])\n",
    "        picks[\"tp_prob\"] = picks[\"tp_prob\"].map(lambda x: [round(float(i), 2) for i in x.strip(\"[]\").split(\" \") if i != \"\"])\n",
    "        picks[\"ts_prob\"] = picks[\"ts_prob\"].map(lambda x: [round(float(i), 2) for i in x.strip(\"[]\").split(\" \") if i != \"\"])\n",
    "        return picks\n",
    "        \n",
    "    def convert_data(meta, dims):\n",
    "        data = []\n",
    "        locs = []\n",
    "        phase_type = []\n",
    "        phase_weight = []\n",
    "        for i in range(len(meta)):\n",
    "            for tp, tp_prob in zip(meta.iloc[i][\"itp\"], meta.iloc[i][\"tp_prob\"]):\n",
    "                data.append(tp)\n",
    "                locs.append(meta.iloc[i][dims].values.astype(\"float\"))\n",
    "                phase_type.append(\"p\")\n",
    "                phase_weight.append(tp_prob)\n",
    "            for ts, ts_prob in zip(meta.iloc[i][\"its\"], meta.iloc[i][\"ts_prob\"]):\n",
    "                data.append(ts)\n",
    "                locs.append(meta.iloc[i][dims].values.astype(\"float\"))\n",
    "                phase_type.append(\"s\")\n",
    "                phase_weight.append(ts_prob)\n",
    "        locs = np.array(locs)\n",
    "        data = np.array(data)[:, np.newaxis]\n",
    "        phase_weight = np.array(phase_weight)[:, np.newaxis]\n",
    "        return data, locs, phase_type, phase_weight\n",
    "    \n",
    "    dt = 0.01\n",
    "    vp = 6.0\n",
    "    vs = vp/1.75\n",
    "    dims = ['x(km)', 'y(km)']\n",
    "    \n",
    "    picks = read_picks(pick_list)\n",
    "    \n",
    "    with open(station_list) as fp:\n",
    "        stations = pd.read_csv(fp, delimiter=\"\\t\")\n",
    "    num_sta = len(stations)\n",
    "#     events = pd.read_csv(\"events.csv\", delimiter=\"\\t\")\n",
    "    meta = pd.merge(stations, picks, on=\"station\")\n",
    "    time_intervals = sorted(list(set(meta[\"time\"])))\n",
    "    \n",
    "    ## \n",
    "    eq_t = []\n",
    "    eq_loc = []\n",
    "    eq_std = []\n",
    "    eq_prob = []\n",
    "    # data_prob = []\n",
    "    # data_score = []\n",
    "\n",
    "    num = 0\n",
    "    for t in tqdm(time_intervals):\n",
    "\n",
    "        data, locs, phase_type, phase_weight = convert_data(meta[meta[\"time\"] == t], dims)\n",
    "\n",
    "        num_event = max(int(len(data)/num_sta*3.0), 4)\n",
    "        centers_init = np.vstack([np.ones(num_event)*np.mean(locs[:,0]),\n",
    "                                  np.ones(num_event)*np.mean(locs[:,1]),\n",
    "        #                           np.ones(num_event)*0.0,\n",
    "                                  np.linspace(data.min(), data.max(), num_event)]).T # n_eve, n_dim(x, y, z) + 1(t)\n",
    "\n",
    "        gmm = mixture.GaussianMixture(n_components=num_event, covariance_type='full', \n",
    "                                      centers_init=centers_init.copy(), station_locs=locs, \n",
    "                                      phase_type=phase_type, phase_weight=phase_weight).fit(data) \n",
    "        pred = gmm.predict(data) \n",
    "        prob = gmm.predict_proba(data)\n",
    "        prob_eq = prob.mean(axis=0)\n",
    "        std_eq = gmm.covariances_.squeeze()\n",
    "        ii = np.array([True if len(data[pred==i, 0]) > max(num_sta//2, 4) else False for i in range(len(prob_eq))]) & (prob_eq > 1/num_event) & (std_eq < 40)\n",
    "        prob_data = prob[range(len(data)), pred]\n",
    "        score_data = gmm.score_samples(data)\n",
    "\n",
    "        result = gmm.centers_[ii,:]\n",
    "        time = pd.Timestamp(t) + result[:, -1].astype('timedelta64[s]')\n",
    "        loc = result[:, :-1]\n",
    "\n",
    "        eq_t.append(time)\n",
    "        eq_loc.append(loc)\n",
    "        eq_std.append(std_eq[ii])\n",
    "        eq_prob.append(prob_eq[ii]*num_event)\n",
    "#         break\n",
    "    \n",
    "    eq_t = np.hstack(eq_t)\n",
    "    eq_loc = np.vstack(eq_loc)\n",
    "    eq_std = np.hstack(eq_std)\n",
    "    eq_prob = np.hstack(eq_prob)\n",
    "    # data_prob = np.hstack(data_prob)\n",
    "    # data_score = np.hstack(data_score)\n",
    "    catalog = {}\n",
    "    catalog[\"time\"] = eq_t\n",
    "    for i, k in enumerate(dims):\n",
    "        catalog[k] = eq_loc[:,i]\n",
    "    catalog[\"prob\"] = eq_prob\n",
    "    catalog[\"std\"] = eq_std\n",
    "    catalog = pd.DataFrame(catalog, columns=[\"time\"]+dims+[\"prob\", \"std\"])\n",
    "    \n",
    "    with open(catalog_path, 'w') as fp:\n",
    "        catalog.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                       float_format=\"%.3f\",\n",
    "                       date_format='%Y-%m-%dT%H:%M:%S.%f')\n",
    "    \n",
    "    with open(os.path.join(data_path, f\"catalog_{idx[0]:04d}.csv\"), 'w') as fp:\n",
    "        catalog.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                       float_format=\"%.3f\",\n",
    "                       date_format='%Y-%m-%dT%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [08:04<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "gmma(\"test\", [1], \"picks.csv\", \"stations.csv\", \"catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmma_op = comp.func_to_container_op(gmma, \n",
    "                                     base_image='python:3.8',\n",
    "                                     packages_to_install= [\n",
    "                                         \"tqdm\",\n",
    "                                         \"pandas\",\n",
    "                                         \"numpy\",\n",
    "                                         \"gmma\"\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_catalog(data_path, \n",
    "                    catalog_path: OutputPath(str)):\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import os\n",
    "    catalog_list = list(sorted(glob(os.path.join(data_path, \"catalog_*.csv\"))))\n",
    "    tmp = []\n",
    "    for c in catalog_list:\n",
    "        with open(c, 'r') as fp:\n",
    "            tmp.append(pd.read_csv(fp, sep=\"\\t\"))\n",
    "    catalog = pd.concat(tmp)\n",
    "    with open(catalog_path, \"w\") as fp:\n",
    "        catalog.to_csv(fp, sep=\"\\t\", index=False, \n",
    "                       float_format=\"%.3f\",\n",
    "                       date_format='%Y-%m-%dT%H:%M:%S.%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_catalog(\"./test\", \"catalog_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_op = comp.func_to_container_op(combine_catalog, \n",
    "                                       base_image='python:3.8',\n",
    "                                       packages_to_install= [\n",
    "                                           \"pandas\",\n",
    "                                       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(name='QuakeFlow', description='')\n",
    "def quakeflow_pipeline(data_path: str, s3_url:str=\"localhost:9000\", secure:bool=False):\n",
    "    \n",
    "#     s3_url = \"10.3.252.218\"\n",
    "#     s3_url = \"127.0.0.1\"\n",
    "#     pvop = dsl.VolumeOp(name=\"Create_volume\",\n",
    "#                        resource_name=\"data-volume\", \n",
    "#                        size=\"10Gi\", \n",
    "#                        modes=dsl.VOLUME_MODE_RWO).volume\n",
    "    pvop = dsl.PipelineVolume(pvc=\"quakeflow-smw46-data-volume\")\n",
    "    \n",
    "    config = config_op(data_path)#.add_pvolumes({data_path: vop.volume})\n",
    "        \n",
    "    events = download_events_op(data_path, config.outputs[\"config\"])#.add_pvolumes({data_path: config.pvolume})\n",
    "    \n",
    "    stations = download_stations_op(data_path, config.outputs[\"config\"])#.add_pvolumes({data_path: config.pvolume})\n",
    "\n",
    "    with kfp.dsl.ParallelFor(config.outputs[\"output\"]) as idx:\n",
    "        download_op_ = download_waveform_op(data_path, idx, \n",
    "                                            config.outputs[\"config\"], \n",
    "                                            config.outputs[\"datetime\"], \n",
    "                                            stations.outputs[\"station\"]\n",
    "                                           ).add_pvolumes({data_path: pvop}).after(stations)\n",
    "        phasenet_op_ = phasenet_op(download_op_.outputs[\"output\"], \n",
    "                                   download_op_.outputs[\"fname_list\"], \n",
    "                                   stations.outputs[\"station_list\"]\n",
    "                                   ).add_pvolumes({data_path: pvop}).after(download_op_)\n",
    "        gmma_op_ = gmma_op(data_path, idx,\n",
    "                          phasenet_op_.outputs[\"picks\"],\n",
    "                          stations.outputs[\"station_list\"]\n",
    "                          ).add_pvolumes({data_path: pvop}).after(phasenet_op_)\n",
    "\n",
    "    combine_op_ = combine_op(data_path).add_pvolumes({data_path: pvop}).after(gmma_op_)\n",
    "    combine_op_.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='https://91eef1af6962.ngrok.io/')\n",
    "# client = kfp.Client(host='553ab00ece5a86e5-dot-us-west1.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://91eef1af6962.ngrok.io//#/experiments/details/b30accf3-c169-4c6c-a576-1da18951d35e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://91eef1af6962.ngrok.io//#/runs/details/17df75a6-d056-468c-bad8-9ec6f74036b2\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'QuakeFlow'\n",
    "pipeline_func = quakeflow_pipeline\n",
    "run_name = pipeline_func.__name__ + '_run'\n",
    "\n",
    "arguments = {\"data_path\": \"/tmp/\",\n",
    "#              \"s3_url\": \"18663dd7908c.ngrok.io\",\n",
    "#              \"secure\": True\n",
    "#              \"s3_url\": \"10.3.248.45:9000\",\n",
    "#              \"secure\": False\n",
    "            }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "results = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                               experiment_name=experiment_name, \n",
    "                                               run_name=run_name, \n",
    "                                               arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
